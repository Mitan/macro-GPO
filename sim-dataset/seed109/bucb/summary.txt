Physical State
[[ 1.05  0.6 ]
 [ 1.1   0.6 ]
 [ 1.15  0.6 ]
 [ 1.2   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.15  0.8 ]
 [ 1.1   0.8 ]
 [ 1.05  0.8 ]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.05  0.6 ]
 [ 1.1   0.6 ]
 [ 1.15  0.6 ]
 [ 1.2   0.6 ]]
Measurements
[ 0.33235771  0.21902674  0.10119739  0.02863632 -0.00336014  0.28825795
  0.57096165  0.81327728  1.02405907  1.03649482  1.0135924   0.9514575
  0.85210995  1.04647303  1.2526742   1.45522654  1.65778737  1.80320783
  1.86870985  1.83963207  1.7303562 ]
===============================================Measurements Collected
[array([ 0.21902674,  0.10119739,  0.02863632, -0.00336014]), array([ 0.28825795,  0.57096165,  0.81327728,  1.02405907]), array([ 1.03649482,  1.0135924 ,  0.9514575 ,  0.85210995]), array([ 1.04647303,  1.2526742 ,  1.45522654,  1.65778737]), array([ 1.80320783,  1.86870985,  1.83963207,  1.7303562 ])]
Base measurements collected
[array([ 0.21902674,  0.10119739,  0.02863632, -0.00336014]), array([ 0.28825795,  0.57096165,  0.81327728,  1.02405907]), array([ 1.03649482,  1.0135924 ,  0.9514575 ,  0.85210995]), array([ 1.04647303,  1.2526742 ,  1.45522654,  1.65778737]), array([ 1.80320783,  1.86870985,  1.83963207,  1.7303562 ])]
Total accumulated reward = 19.5497780173
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.34550031258079017, 3.0420562676605574, 6.8957109315725189, 12.307872073783052, 19.549778017329878]
Normalized Reward history [0.24722286178613195, 2.8455013660712414, 6.6008785791885449, 11.91476227060442, 19.058390763356591]
