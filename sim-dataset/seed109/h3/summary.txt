Physical State
[[ 0.8   1.15]
 [ 0.8   1.1 ]
 [ 0.8   1.05]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   1.05]
 [ 0.8   1.1 ]
 [ 0.8   1.15]
 [ 0.8   1.2 ]
 [ 0.8   1.15]
 [ 0.8   1.1 ]
 [ 0.8   1.05]
 [ 0.8   1.  ]
 [ 0.8   1.05]
 [ 0.8   1.1 ]
 [ 0.8   1.15]
 [ 0.8   1.2 ]
 [ 0.8   1.15]
 [ 0.8   1.1 ]
 [ 0.8   1.05]
 [ 0.8   1.  ]]
Measurements
[ 0.33235771  0.44038426  0.5040647   0.528201    0.5063099   0.60538678
  0.6627286   0.65974037  0.55145943  0.65974037  0.6627286   0.60538678
  0.5063099   0.60538678  0.6627286   0.65974037  0.55145943  0.65974037
  0.6627286   0.60538678  0.5063099 ]
===============================================Measurements Collected
[array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ]), array([ 0.60538678,  0.6627286 ,  0.65974037,  0.55145943]), array([ 0.65974037,  0.6627286 ,  0.60538678,  0.5063099 ]), array([ 0.60538678,  0.6627286 ,  0.65974037,  0.55145943]), array([ 0.65974037,  0.6627286 ,  0.60538678,  0.5063099 ])]
Base measurements collected
[array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ]), array([ 0.60538678,  0.6627286 ,  0.65974037,  0.55145943]), array([ 0.65974037,  0.6627286 ,  0.60538678,  0.5063099 ]), array([ 0.60538678,  0.6627286 ,  0.65974037,  0.55145943]), array([ 0.65974037,  0.6627286 ,  0.60538678,  0.5063099 ])]
Total accumulated reward = 11.8059215301
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [1.9789598563613462, 4.4582750385993135, 6.8924406932112472, 9.3717558754492138, 11.805921530061148]
Normalized Reward history [1.880682405566688, 4.2617201370099975, 6.5976083408272732, 8.9786460722705819, 11.314534276087858]
