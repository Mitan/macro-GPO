Physical State
[[ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   1.05]
 [ 0.8   1.1 ]
 [ 0.8   1.15]
 [ 0.8   1.2 ]
 [ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Measurements
[ 0.33235771  0.44038426  0.5040647   0.528201    0.5063099   0.60538678
  0.6627286   0.65974037  0.55145943  0.68551891  0.70532294  0.60541307
  0.39547984  0.3628882   0.33749306  0.32066352  0.33235771  0.44038426
  0.5040647   0.528201    0.5063099 ]
===============================================Measurements Collected
[array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ]), array([ 0.60538678,  0.6627286 ,  0.65974037,  0.55145943]), array([ 0.68551891,  0.70532294,  0.60541307,  0.39547984]), array([ 0.3628882 ,  0.33749306,  0.32066352,  0.33235771]), array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ])]
Base measurements collected
[array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ]), array([ 0.60538678,  0.6627286 ,  0.65974037,  0.55145943]), array([ 0.68551891,  0.70532294,  0.60541307,  0.39547984]), array([ 0.3628882 ,  0.33749306,  0.32066352,  0.33235771]), array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ])]
Total accumulated reward = 10.1823721314
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [1.9789598563613462, 4.4582750385993135, 6.8500098009311765, 8.2034122750295406, 10.182372131390887]
Normalized Reward history [1.880682405566688, 4.2617201370099975, 6.5551774485472025, 7.8103024718509078, 9.6909848774175948]
