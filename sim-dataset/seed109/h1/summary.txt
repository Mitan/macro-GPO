Physical State
[[ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Measurements
[ 0.33235771  0.32066352  0.33749306  0.3628882   0.39547984  0.41317229
  0.40981137  0.3794946   0.3312803   0.3794946   0.40981137  0.41317229
  0.39547984  0.41317229  0.40981137  0.3794946   0.3312803   0.3794946
  0.40981137  0.41317229  0.39547984]
===============================================Measurements Collected
[array([ 0.32066352,  0.33749306,  0.3628882 ,  0.39547984]), array([ 0.41317229,  0.40981137,  0.3794946 ,  0.3312803 ]), array([ 0.3794946 ,  0.40981137,  0.41317229,  0.39547984]), array([ 0.41317229,  0.40981137,  0.3794946 ,  0.3312803 ]), array([ 0.3794946 ,  0.40981137,  0.41317229,  0.39547984])]
Base measurements collected
[array([ 0.32066352,  0.33749306,  0.3628882 ,  0.39547984]), array([ 0.41317229,  0.40981137,  0.3794946 ,  0.3312803 ]), array([ 0.3794946 ,  0.40981137,  0.41317229,  0.39547984]), array([ 0.41317229,  0.40981137,  0.3794946 ,  0.3312803 ]), array([ 0.3794946 ,  0.40981137,  0.41317229,  0.39547984])]
Total accumulated reward = 7.67995793601
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [1.416524608204258, 2.9502831707275128, 4.548241272105269, 6.0819998346285242, 7.6799579360062804]
Normalized Reward history [1.3182471574095997, 2.7537282691381968, 4.253408919721295, 5.6888900314498922, 7.1885706820329904]
