Physical State
[[ 1.    0.25]
 [ 1.    0.3 ]
 [ 1.    0.35]
 [ 1.    0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.55]
 [ 1.    0.5 ]
 [ 1.    0.45]
 [ 1.    0.4 ]
 [ 1.    0.35]
 [ 1.    0.3 ]
 [ 1.    0.25]
 [ 1.    0.2 ]
 [ 1.    0.25]
 [ 1.    0.3 ]
 [ 1.    0.35]
 [ 1.    0.4 ]]
Measurements
[ 0.33235771  0.40391712  0.51736935  0.67365833  0.85210995  1.04647303
  1.2526742   1.45522654  1.65778737  1.86894226  2.09053738  2.30122016
  2.46436642  2.57102653  2.58645338  2.51945066  2.39494731  2.51945066
  2.58645338  2.57102653  2.46436642]
===============================================Measurements Collected
[array([ 0.40391712,  0.51736935,  0.67365833,  0.85210995]), array([ 1.04647303,  1.2526742 ,  1.45522654,  1.65778737]), array([ 1.86894226,  2.09053738,  2.30122016,  2.46436642]), array([ 2.57102653,  2.58645338,  2.51945066,  2.39494731]), array([ 2.51945066,  2.58645338,  2.57102653,  2.46436642])]
Base measurements collected
[array([ 0.40391712,  0.51736935,  0.67365833,  0.85210995]), array([ 1.04647303,  1.2526742 ,  1.45522654,  1.65778737]), array([ 1.86894226,  2.09053738,  2.30122016,  2.46436642]), array([ 2.57102653,  2.58645338,  2.51945066,  2.39494731]), array([ 2.51945066,  2.58645338,  2.57102653,  2.46436642])]
Total accumulated reward = 36.7974569742
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [2.4470547506238365, 7.8592158928343689, 16.584282101661287, 26.656159981102206, 36.79745697416147]
Normalized Reward history [2.3487772998291785, 7.662660991245053, 16.28944974927731, 26.263050177923567, 36.306069720188177]
