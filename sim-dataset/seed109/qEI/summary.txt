Physical State
[[ 1.15  0.8 ]
 [ 1.1   0.8 ]
 [ 1.05  0.8 ]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.2   0.75]
 [ 1.2   0.7 ]
 [ 1.2   0.65]
 [ 1.2   0.6 ]
 [ 1.2   0.65]
 [ 1.2   0.7 ]
 [ 1.2   0.75]
 [ 1.2   0.8 ]
 [ 1.15  0.8 ]
 [ 1.1   0.8 ]
 [ 1.05  0.8 ]
 [ 1.    0.8 ]]
Measurements
[ 0.33235771  0.21902674  0.10119739  0.02863632 -0.00336014  0.28825795
  0.57096165  0.81327728  1.02405907  1.19370503  1.35938765  1.52945385
  1.7303562   1.52945385  1.35938765  1.19370503  1.02405907  1.03649482
  1.0135924   0.9514575   0.85210995]
===============================================Measurements Collected
[array([ 0.21902674,  0.10119739,  0.02863632, -0.00336014]), array([ 0.28825795,  0.57096165,  0.81327728,  1.02405907]), array([ 1.19370503,  1.35938765,  1.52945385,  1.7303562 ]), array([ 1.52945385,  1.35938765,  1.19370503,  1.02405907]), array([ 1.03649482,  1.0135924 ,  0.9514575 ,  0.85210995])]
Base measurements collected
[array([ 0.21902674,  0.10119739,  0.02863632, -0.00336014]), array([ 0.28825795,  0.57096165,  0.81327728,  1.02405907]), array([ 1.19370503,  1.35938765,  1.52945385,  1.7303562 ]), array([ 1.52945385,  1.35938765,  1.19370503,  1.02405907]), array([ 1.03649482,  1.0135924 ,  0.9514575 ,  0.85210995])]
Total accumulated reward = 17.8152192712
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [0.34550031258079017, 3.0420562676605574, 8.8549590001190737, 13.961564607304826, 17.815219271216787]
Normalized Reward history [0.24722286178613195, 2.8455013660712414, 8.5601266477350997, 13.568454804126194, 17.323832017243497]
