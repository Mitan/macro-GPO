Physical State
[[ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Measurements
[ 0.33235771  0.44038426  0.5040647   0.528201    0.5063099   0.528201
  0.5040647   0.44038426  0.33235771  0.44038426  0.5040647   0.528201
  0.5063099   0.528201    0.5040647   0.44038426  0.33235771  0.44038426
  0.5040647   0.528201    0.5063099 ]
===============================================Measurements Collected
[array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ]), array([ 0.528201  ,  0.5040647 ,  0.44038426,  0.33235771]), array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ]), array([ 0.528201  ,  0.5040647 ,  0.44038426,  0.33235771]), array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ])]
Base measurements collected
[array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ]), array([ 0.528201  ,  0.5040647 ,  0.44038426,  0.33235771]), array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ]), array([ 0.528201  ,  0.5040647 ,  0.44038426,  0.33235771]), array([ 0.44038426,  0.5040647 ,  0.528201  ,  0.5063099 ])]
Total accumulated reward = 9.54689488263
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [1.9789598563613462, 3.7839675131348605, 5.7629273694962064, 7.5679350262697209, 9.5468948826310669]
Normalized Reward history [1.880682405566688, 3.5874126115455436, 5.4680950171122316, 7.1748252230910872, 9.0555076286577751]
