Physical State
[[ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.25  1.  ]
 [ 1.3   1.  ]
 [ 1.35  1.  ]
 [ 1.4   1.  ]
 [ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]
 [ 1.55  1.  ]
 [ 1.5   1.  ]
 [ 1.45  1.  ]
 [ 1.4   1.  ]
 [ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]]
Measurements
[-0.00123181 -0.26791519 -0.44761883 -0.51423809 -0.42894904 -0.21444119
  0.09202031  0.43329578  0.74065477  0.92834007  0.98756971  0.90868099
  0.70925859  0.90868099  0.98756971  0.92834007  0.74065477  0.92834007
  0.98756971  0.90868099  0.70925859]
===============================================Measurements Collected
[array([-0.26791519, -0.44761883, -0.51423809, -0.42894904]), array([-0.21444119,  0.09202031,  0.43329578,  0.74065477]), array([ 0.92834007,  0.98756971,  0.90868099,  0.70925859]), array([ 0.90868099,  0.98756971,  0.92834007,  0.74065477]), array([ 0.92834007,  0.98756971,  0.90868099,  0.70925859])]
Base measurements collected
[array([-0.26791519, -0.44761883, -0.51423809, -0.42894904]), array([-0.21444119,  0.09202031,  0.43329578,  0.74065477]), array([ 0.92834007,  0.98756971,  0.90868099,  0.70925859]), array([ 0.90868099,  0.98756971,  0.92834007,  0.74065477]), array([ 0.92834007,  0.98756971,  0.90868099,  0.70925859])]
Total accumulated reward = 10.0257527736
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-1.6587211496808107, -0.60719147324715017, 2.9266578800349974, 6.4919034203673203, 10.025752773649469]
Normalized Reward history [-3.1896175779245333, -3.6689843297345952, -1.6660314046961702, 0.36831770739243019, 2.3712706324308552]
