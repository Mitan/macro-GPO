Physical State
[[ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]
 [ 0.8   1.35]
 [ 0.8   1.3 ]
 [ 0.8   1.25]
 [ 0.8   1.2 ]
 [ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]]
Measurements
[ -1.23180961e-03   1.51493140e-01   4.45556091e-01   8.56873291e-01
   1.32461607e+00   1.73928006e+00   2.01194726e+00   2.07533303e+00
   1.92434741e+00   1.66137051e+00   1.34430581e+00   9.95530023e-01
   6.58292036e-01   1.08753417e+00   1.34077378e+00   1.40519645e+00
   1.32998256e+00   1.45671051e+00   1.48988769e+00   1.44713093e+00
   1.32461607e+00]
===============================================Measurements Collected
[array([ 0.15149314,  0.44555609,  0.85687329,  1.32461607]), array([ 1.73928006,  2.01194726,  2.07533303,  1.92434741]), array([ 1.66137051,  1.34430581,  0.99553002,  0.65829204]), array([ 1.08753417,  1.34077378,  1.40519645,  1.32998256]), array([ 1.45671051,  1.48988769,  1.44713093,  1.32461607])]
Base measurements collected
[array([ 0.15149314,  0.44555609,  0.85687329,  1.32461607]), array([ 1.73928006,  2.01194726,  2.07533303,  1.92434741]), array([ 1.66137051,  1.34430581,  0.99553002,  0.65829204]), array([ 1.08753417,  1.34077378,  1.40519645,  1.32998256]), array([ 1.45671051,  1.48988769,  1.44713093,  1.32461607])]
Total accumulated reward = 26.0707769005
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [2.7785385930885029, 10.529446359109183, 15.188944740315794, 20.352431691462655, 26.070776900478709]
Normalized Reward history [1.2476421648447806, 7.4676535026217374, 10.596255455584625, 14.228845978487762, 18.41629475926009]
