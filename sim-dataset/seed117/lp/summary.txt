Physical State
[[ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Measurements
[-0.00123181  0.2896717   0.56187706  0.8055583   0.99913956  0.8055583
  0.56187706  0.2896717  -0.00123181  0.2896717   0.56187706  0.8055583
  0.99913956  0.8055583   0.56187706  0.2896717  -0.00123181  0.2896717
  0.56187706  0.8055583   0.99913956]
===============================================Measurements Collected
[array([ 0.2896717 ,  0.56187706,  0.8055583 ,  0.99913956]), array([ 0.8055583 ,  0.56187706,  0.2896717 , -0.00123181]), array([ 0.2896717 ,  0.56187706,  0.8055583 ,  0.99913956]), array([ 0.8055583 ,  0.56187706,  0.2896717 , -0.00123181]), array([ 0.2896717 ,  0.56187706,  0.8055583 ,  0.99913956])]
Base measurements collected
[array([ 0.2896717 ,  0.56187706,  0.8055583 ,  0.99913956]), array([ 0.8055583 ,  0.56187706,  0.2896717 , -0.00123181]), array([ 0.2896717 ,  0.56187706,  0.8055583 ,  0.99913956]), array([ 0.8055583 ,  0.56187706,  0.2896717 , -0.00123181]), array([ 0.2896717 ,  0.56187706,  0.8055583 ,  0.99913956])]
Total accumulated reward = 11.2804903755
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [2.6562466221023464, 4.312121876682971, 6.9683684987853169, 8.624243753365942, 11.280490375468288]
Normalized Reward history [1.1253501938586241, 1.2503290201955266, 2.3756792140541507, 2.5006580403910532, 3.6260082342496771]
