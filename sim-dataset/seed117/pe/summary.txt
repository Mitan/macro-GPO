Physical State
[[ 1.05  1.4 ]
 [ 1.1   1.4 ]
 [ 1.15  1.4 ]
 [ 1.2   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.8   1.25]
 [ 0.8   1.3 ]
 [ 0.8   1.35]
 [ 0.8   1.4 ]
 [ 0.85  1.4 ]
 [ 0.9   1.4 ]
 [ 0.95  1.4 ]
 [ 1.    1.4 ]
 [ 1.05  1.4 ]
 [ 1.1   1.4 ]
 [ 1.15  1.4 ]
 [ 1.2   1.4 ]]
Measurements
[ -1.23180961e-03   1.51493140e-01   4.45556091e-01   8.56873291e-01
   1.32461607e+00   1.44713093e+00   1.48988769e+00   1.45671051e+00
   1.32998256e+00   1.40519645e+00   1.34077378e+00   1.08753417e+00
   6.58292036e-01   9.95530023e-01   1.34430581e+00   1.66137051e+00
   1.92434741e+00   2.12982963e+00   2.23879344e+00   2.24859633e+00
   2.17788343e+00]
===============================================Measurements Collected
[array([ 0.15149314,  0.44555609,  0.85687329,  1.32461607]), array([ 1.44713093,  1.48988769,  1.45671051,  1.32998256]), array([ 1.40519645,  1.34077378,  1.08753417,  0.65829204]), array([ 0.99553002,  1.34430581,  1.66137051,  1.92434741]), array([ 2.12982963,  2.23879344,  2.24859633,  2.17788343])]
Base measurements collected
[array([ 0.15149314,  0.44555609,  0.85687329,  1.32461607]), array([ 1.44713093,  1.48988769,  1.45671051,  1.32998256]), array([ 1.40519645,  1.34077378,  1.08753417,  0.65829204]), array([ 0.99553002,  1.34430581,  1.66137051,  1.92434741]), array([ 2.12982963,  2.23879344,  2.24859633,  2.17788343])]
Total accumulated reward = 27.7147033095
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [2.7785385930885029, 8.502250289883186, 12.994046718987288, 18.919600475549373, 27.71470330951411]
Normalized Reward history [1.2476421648447806, 5.4404574333957409, 8.4013574342561199, 12.796014762574483, 20.060221168295499]
