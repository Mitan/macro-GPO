Physical State
[[ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Measurements
[ -1.23180961e-03   1.51493140e-01   4.45556091e-01   8.56873291e-01
   1.32461607e+00   1.73928006e+00   2.01194726e+00   2.07533303e+00
   1.92434741e+00   2.07533303e+00   2.01194726e+00   1.73928006e+00
   1.32461607e+00   1.73928006e+00   2.01194726e+00   2.07533303e+00
   1.92434741e+00   2.07533303e+00   2.01194726e+00   1.73928006e+00
   1.32461607e+00]
===============================================Measurements Collected
[array([ 0.15149314,  0.44555609,  0.85687329,  1.32461607]), array([ 1.73928006,  2.01194726,  2.07533303,  1.92434741]), array([ 2.07533303,  2.01194726,  1.73928006,  1.32461607]), array([ 1.73928006,  2.01194726,  2.07533303,  1.92434741]), array([ 2.07533303,  2.01194726,  1.73928006,  1.32461607])]
Base measurements collected
[array([ 0.15149314,  0.44555609,  0.85687329,  1.32461607]), array([ 1.73928006,  2.01194726,  2.07533303,  1.92434741]), array([ 2.07533303,  2.01194726,  1.73928006,  1.32461607]), array([ 1.73928006,  2.01194726,  2.07533303,  1.92434741]), array([ 2.07533303,  2.01194726,  1.73928006,  1.32461607])]
Total accumulated reward = 32.582706975
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [2.7785385930885029, 10.529446359109183, 17.680622784038516, 25.431530550059193, 32.582706974988525]
Normalized Reward history [1.2476421648447806, 7.4676535026217374, 13.087933499307347, 19.307944837084303, 24.92822483376991]
