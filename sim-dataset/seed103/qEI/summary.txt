Physical State
[[ 1.4   1.05]
 [ 1.4   1.1 ]
 [ 1.4   1.15]
 [ 1.4   1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]
 [ 1.25  1.  ]
 [ 1.3   1.  ]
 [ 1.35  1.  ]
 [ 1.4   1.  ]
 [ 1.4   1.05]
 [ 1.4   1.1 ]
 [ 1.4   1.15]
 [ 1.4   1.2 ]]
Measurements
[ 1.95175563  2.15426418  2.23305472  2.17042829  1.96110394  1.9537866
  1.80283083  1.53877574  1.21457201  1.37653953  1.4374676   1.40724788
  1.29566023  0.94047923  0.5643845   0.17849808 -0.16860942 -0.21026655
 -0.22939349 -0.21742696 -0.19036757]
===============================================Measurements Collected
[array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 1.9537866 ,  1.80283083,  1.53877574,  1.21457201]), array([ 1.37653953,  1.4374676 ,  1.40724788,  1.29566023]), array([ 0.94047923,  0.5643845 ,  0.17849808, -0.16860942]), array([-0.21026655, -0.22939349, -0.21742696, -0.19036757])]
Base measurements collected
[array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 1.9537866 ,  1.80283083,  1.53877574,  1.21457201]), array([ 1.37653953,  1.4374676 ,  1.40724788,  1.29566023]), array([ 0.94047923,  0.5643845 ,  0.17849808, -0.16860942]), array([-0.21026655, -0.22939349, -0.21742696, -0.19036757])]
Total accumulated reward = 21.2130293602
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [8.5188511305504075, 15.028816307057506, 20.545731550300012, 22.060483941042058, 21.21302936020593]
Normalized Reward history [6.938128968904592, 11.867371983765874, 15.803565065362566, 15.737595294458799, 13.309418551976854]
