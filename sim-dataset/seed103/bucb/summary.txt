Physical State
[[ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]]
Measurements
[ 1.95175563  1.80801021  1.54270926  1.16258247  0.72176184  1.16258247
  1.54270926  1.80801021  1.95175563  2.15426418  2.23305472  2.17042829
  1.96110394  1.9537866   1.80283083  1.53877574  1.21457201  1.37653953
  1.4374676   1.40724788  1.29566023]
===============================================Measurements Collected
[array([ 1.80801021,  1.54270926,  1.16258247,  0.72176184]), array([ 1.16258247,  1.54270926,  1.80801021,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 1.9537866 ,  1.80283083,  1.53877574,  1.21457201]), array([ 1.37653953,  1.4374676 ,  1.40724788,  1.29566023])]
Base measurements collected
[array([ 1.80801021,  1.54270926,  1.16258247,  0.72176184]), array([ 1.16258247,  1.54270926,  1.80801021,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 1.9537866 ,  1.80283083,  1.53877574,  1.21457201]), array([ 1.37653953,  1.4374676 ,  1.40724788,  1.29566023])]
Total accumulated reward = 32.2458528848
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [5.2350637719040938, 11.700121334458029, 20.218972465008434, 26.728937641515532, 32.245852884758037]
Normalized Reward history [3.6543416102582782, 8.5386770111663992, 15.476805980070992, 20.406048994932274, 24.342242076528965]
