Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 1.95175563  2.15426418  2.23305472  2.17042829  1.96110394  2.17042829
  2.23305472  2.15426418  1.95175563  2.15426418  2.23305472  2.17042829
  1.96110394  2.17042829  2.23305472  2.15426418  1.95175563  2.15426418
  2.23305472  2.17042829  1.96110394]
===============================================Measurements Collected
[array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 2.17042829,  2.23305472,  2.15426418,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 2.17042829,  2.23305472,  2.15426418,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Base measurements collected
[array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 2.17042829,  2.23305472,  2.15426418,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 2.17042829,  2.23305472,  2.15426418,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Total accumulated reward = 42.5755590326
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [8.5188511305504075, 17.028353951017579, 25.547205081567988, 34.056707902035157, 42.575559032585566]
Normalized Reward history [6.938128968904592, 13.866909627725946, 20.805038596630538, 27.733819255451891, 34.671948224356484]
