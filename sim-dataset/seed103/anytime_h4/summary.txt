Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 1.95175563  1.66112116  1.32726946  0.97296152  0.60691191  0.28035932
 -0.01138304 -0.2324964  -0.38569587 -0.2324964  -0.01138304  0.28035932
  0.60691191  0.97296152  1.32726946  1.66112116  1.95175563  2.15426418
  2.23305472  2.17042829  1.96110394]
===============================================Measurements Collected
[array([ 1.66112116,  1.32726946,  0.97296152,  0.60691191]), array([ 0.28035932, -0.01138304, -0.2324964 , -0.38569587]), array([-0.2324964 , -0.01138304,  0.28035932,  0.60691191]), array([ 0.97296152,  1.32726946,  1.66112116,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Base measurements collected
[array([ 1.66112116,  1.32726946,  0.97296152,  0.60691191]), array([ 0.28035932, -0.01138304, -0.2324964 , -0.38569587]), array([-0.2324964 , -0.01138304,  0.28035932,  0.60691191]), array([ 0.97296152,  1.32726946,  1.66112116,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Total accumulated reward = 19.2943987481
Nodes Expanded per stage
[118778, 110314, 22090, 6001, 5]
Total nodes expanded = 257188
Reward history [4.5682640468966458, 4.2190480588956829, 4.862439857169222, 10.775547617580326, 19.294398748130732]
Normalized Reward history [2.9875418852508302, 1.0576037356040522, 0.12027337223177559, 4.4526589709970628, 11.390787939901655]
