Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.05  1.2 ]
 [ 1.1   1.2 ]
 [ 1.15  1.2 ]
 [ 1.2   1.2 ]
 [ 1.2   1.15]
 [ 1.2   1.1 ]
 [ 1.2   1.05]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 1.95175563  1.66112116  1.32726946  0.97296152  0.60691191  0.70159439
  0.75550685  0.77339921  0.71355172  0.85433244  1.00207562  1.15421574
  1.29566023  1.60196019  1.82573399  1.95059338  1.95175563  2.15426418
  2.23305472  2.17042829  1.96110394]
===============================================Measurements Collected
[array([ 1.66112116,  1.32726946,  0.97296152,  0.60691191]), array([ 0.70159439,  0.75550685,  0.77339921,  0.71355172]), array([ 0.85433244,  1.00207562,  1.15421574,  1.29566023]), array([ 1.60196019,  1.82573399,  1.95059338,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Base measurements collected
[array([ 1.66112116,  1.32726946,  0.97296152,  0.60691191]), array([ 0.70159439,  0.75550685,  0.77339921,  0.71355172]), array([ 0.85433244,  1.00207562,  1.15421574,  1.29566023]), array([ 1.60196019,  1.82573399,  1.95059338,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Total accumulated reward = 27.6674945727
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [4.5682640468966458, 7.5123162228958655, 11.818600255643295, 19.148643442161866, 27.667494572712272]
Normalized Reward history [2.9875418852508302, 4.3508718996042344, 7.0764337707058482, 12.825754795578604, 19.763883764483197]
