Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 1.95175563  1.80801021  1.54270926  1.16258247  0.72176184  1.16258247
  1.54270926  1.80801021  1.95175563  1.95059338  1.82573399  1.60196019
  1.29566023  1.60196019  1.82573399  1.95059338  1.95175563  2.15426418
  2.23305472  2.17042829  1.96110394]
===============================================Measurements Collected
[array([ 1.80801021,  1.54270926,  1.16258247,  0.72176184]), array([ 1.16258247,  1.54270926,  1.80801021,  1.95175563]), array([ 1.95059338,  1.82573399,  1.60196019,  1.29566023]), array([ 1.60196019,  1.82573399,  1.95059338,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Base measurements collected
[array([ 1.80801021,  1.54270926,  1.16258247,  0.72176184]), array([ 1.16258247,  1.54270926,  1.80801021,  1.95175563]), array([ 1.95059338,  1.82573399,  1.60196019,  1.29566023]), array([ 1.60196019,  1.82573399,  1.95059338,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Total accumulated reward = 34.2229634405
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [5.2350637719040938, 11.700121334458029, 18.374069123452841, 25.704112309971411, 34.222963440521816]
Normalized Reward history [3.6543416102582782, 8.5386770111663992, 13.631902638515395, 19.381223663388148, 26.319352632292741]
