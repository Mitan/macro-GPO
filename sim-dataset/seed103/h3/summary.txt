Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 1.95175563  1.66112116  1.32726946  0.97296152  0.60691191  0.97296152
  1.32726946  1.66112116  1.95175563  2.15426418  2.23305472  2.17042829
  1.96110394  2.17042829  2.23305472  2.15426418  1.95175563  2.15426418
  2.23305472  2.17042829  1.96110394]
===============================================Measurements Collected
[array([ 1.66112116,  1.32726946,  0.97296152,  0.60691191]), array([ 0.97296152,  1.32726946,  1.66112116,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 2.17042829,  2.23305472,  2.15426418,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Base measurements collected
[array([ 1.66112116,  1.32726946,  0.97296152,  0.60691191]), array([ 0.97296152,  1.32726946,  1.66112116,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394]), array([ 2.17042829,  2.23305472,  2.15426418,  1.95175563]), array([ 2.15426418,  2.23305472,  2.17042829,  1.96110394])]
Total accumulated reward = 36.0285768889
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [4.5682640468966458, 10.481371807307749, 19.000222937858155, 27.509725758325324, 36.028576888875733]
Normalized Reward history [2.9875418852508302, 7.3199274840161177, 14.258056452920709, 21.186837111742062, 28.124966080646654]
