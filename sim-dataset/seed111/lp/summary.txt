Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 0.39174015  0.4801518   0.5861731   0.71524622  0.83771261  0.71524622
  0.5861731   0.4801518   0.39174015  0.4801518   0.5861731   0.71524622
  0.83771261  0.71524622  0.5861731   0.4801518   0.39174015  0.4801518
  0.5861731   0.71524622  0.83771261]
===============================================Measurements Collected
[array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261]), array([ 0.71524622,  0.5861731 ,  0.4801518 ,  0.39174015]), array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261]), array([ 0.71524622,  0.5861731 ,  0.4801518 ,  0.39174015]), array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261])]
Base measurements collected
[array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261]), array([ 0.71524622,  0.5861731 ,  0.4801518 ,  0.39174015]), array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261]), array([ 0.71524622,  0.5861731 ,  0.4801518 ,  0.39174015]), array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261])]
Total accumulated reward = 12.2044737492
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [2.6192837315360387, 4.7925950088430511, 7.4118787403790893, 9.5851900176861022, 12.20447374922214]
Normalized Reward history [1.2909469032365335, 2.1359213522440403, 3.4268682554805738, 4.2718427044880807, 5.5627896077246142]
