Physical State
[[ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.8   1.15]
 [ 0.8   1.1 ]
 [ 0.8   1.05]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]]
Measurements
[ 0.39174015  0.30209275  0.17453192  0.0021096  -0.20620779 -0.30522019
 -0.39254358 -0.42890972 -0.40374602 -0.40972432 -0.46855841 -0.57471592
 -0.70556997 -0.57012414 -0.28072328  0.05121496  0.39174015  0.65438065
  0.81067884  0.88425277  0.92846165]
===============================================Measurements Collected
[array([ 0.30209275,  0.17453192,  0.0021096 , -0.20620779]), array([-0.30522019, -0.39254358, -0.42890972, -0.40374602]), array([-0.40972432, -0.46855841, -0.57471592, -0.70556997]), array([-0.57012414, -0.28072328,  0.05121496,  0.39174015]), array([ 0.65438065,  0.81067884,  0.88425277,  0.92846165])]
Base measurements collected
[array([ 0.30209275,  0.17453192,  0.0021096 , -0.20620779]), array([-0.30522019, -0.39254358, -0.42890972, -0.40374602]), array([-0.40972432, -0.46855841, -0.57471592, -0.70556997]), array([-0.57012414, -0.28072328,  0.05121496,  0.39174015]), array([ 0.65438065,  0.81067884,  0.88425277,  0.92846165])]
Total accumulated reward = -0.546580018718
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.27252647946158282, -1.2578930201929008, -3.4164616322302894, -3.8243539391198289, -0.54658001871818218]
Normalized Reward history [-1.0558103488379222, -3.9145666767919112, -7.4014721171288045, -9.13770125231785, -7.1882641602157085]
