Physical State
[[ 1.4   0.65]
 [ 1.4   0.7 ]
 [ 1.4   0.75]
 [ 1.4   0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.25  1.  ]
 [ 1.3   1.  ]
 [ 1.35  1.  ]
 [ 1.4   1.  ]
 [ 1.4   0.95]
 [ 1.4   0.9 ]
 [ 1.4   0.85]
 [ 1.4   0.8 ]
 [ 1.4   0.75]
 [ 1.4   0.7 ]
 [ 1.4   0.65]
 [ 1.4   0.6 ]
 [ 1.4   0.65]
 [ 1.4   0.7 ]
 [ 1.4   0.75]
 [ 1.4   0.8 ]]
Measurements
[ 0.39174015  0.65438065  0.81067884  0.88425277  0.92846165  0.93459492
  0.97320203  1.00476487  1.05219086  1.01299644  1.02882204  1.10487056
  1.23871066  1.36331213  1.43299327  1.43532982  1.35006208  1.43532982
  1.43299327  1.36331213  1.23871066]
===============================================Measurements Collected
[array([ 0.65438065,  0.81067884,  0.88425277,  0.92846165]), array([ 0.93459492,  0.97320203,  1.00476487,  1.05219086]), array([ 1.01299644,  1.02882204,  1.10487056,  1.23871066]), array([ 1.36331213,  1.43299327,  1.43532982,  1.35006208]), array([ 1.43532982,  1.43299327,  1.36331213,  1.23871066])]
Base measurements collected
[array([ 0.65438065,  0.81067884,  0.88425277,  0.92846165]), array([ 0.93459492,  0.97320203,  1.00476487,  1.05219086]), array([ 1.01299644,  1.02882204,  1.10487056,  1.23871066]), array([ 1.36331213,  1.43299327,  1.43532982,  1.35006208]), array([ 1.43532982,  1.43299327,  1.36331213,  1.23871066])]
Total accumulated reward = 22.6799694654
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [3.2777739204016467, 7.2425266068871981, 11.627926294725038, 17.209623592942048, 22.679969465438752]
Normalized Reward history [1.9494370921021416, 4.5858529502881868, 7.6429158098265209, 11.896276279744026, 16.038285323941224]
