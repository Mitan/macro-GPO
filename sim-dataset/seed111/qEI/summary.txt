Physical State
[[ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.85  0.8 ]
 [ 0.9   0.8 ]
 [ 0.95  0.8 ]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]]
Measurements
[ 0.39174015  0.4801518   0.5861731   0.71524622  0.83771261  0.31073527
 -0.22089091 -0.65802365 -0.93635748 -0.65802365 -0.22089091  0.31073527
  0.83771261  1.26688591  1.55615911  1.69655247  1.7037643   1.50424513
  1.28011408  1.08748789  0.92846165]
===============================================Measurements Collected
[array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261]), array([ 0.31073527, -0.22089091, -0.65802365, -0.93635748]), array([-0.65802365, -0.22089091,  0.31073527,  0.83771261]), array([ 1.26688591,  1.55615911,  1.69655247,  1.7037643 ]), array([ 1.50424513,  1.28011408,  1.08748789,  0.92846165])]
Base measurements collected
[array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261]), array([ 0.31073527, -0.22089091, -0.65802365, -0.93635748]), array([-0.65802365, -0.22089091,  0.31073527,  0.83771261]), array([ 1.26688591,  1.55615911,  1.69655247,  1.7037643 ]), array([ 1.50424513,  1.28011408,  1.08748789,  0.92846165])]
Total accumulated reward = 12.4079508221
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [2.6192837315360387, 1.1147469622272999, 1.3842802814706756, 7.6076420645134464, 12.407950822112058]
Normalized Reward history [1.2909469032365335, -1.5419266943717105, -2.6007302034278399, 2.2942947513154253, 5.7662666806145317]
