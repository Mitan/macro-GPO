Physical State
[[ 1.35  0.6 ]
 [ 1.3   0.6 ]
 [ 1.25  0.6 ]
 [ 1.2   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.25  0.8 ]
 [ 1.3   0.8 ]
 [ 1.35  0.8 ]
 [ 1.4   0.8 ]
 [ 1.4   0.75]
 [ 1.4   0.7 ]
 [ 1.4   0.65]
 [ 1.4   0.6 ]
 [ 1.35  0.6 ]
 [ 1.3   0.6 ]
 [ 1.25  0.6 ]
 [ 1.2   0.6 ]]
Measurements
[ 0.39174015  0.65438065  0.81067884  0.88425277  0.92846165  1.08748789
  1.28011408  1.50424513  1.7037643   1.62014853  1.50333299  1.36291398
  1.23871066  1.36331213  1.43299327  1.43532982  1.35006208  1.37768014
  1.40401708  1.41827744  1.40163107]
===============================================Measurements Collected
[array([ 0.65438065,  0.81067884,  0.88425277,  0.92846165]), array([ 1.08748789,  1.28011408,  1.50424513,  1.7037643 ]), array([ 1.62014853,  1.50333299,  1.36291398,  1.23871066]), array([ 1.36331213,  1.43299327,  1.43532982,  1.35006208]), array([ 1.37768014,  1.40401708,  1.41827744,  1.40163107])]
Base measurements collected
[array([ 0.65438065,  0.81067884,  0.88425277,  0.92846165]), array([ 1.08748789,  1.28011408,  1.50424513,  1.7037643 ]), array([ 1.62014853,  1.50333299,  1.36291398,  1.23871066]), array([ 1.36331213,  1.43299327,  1.43532982,  1.35006208]), array([ 1.37768014,  1.40401708,  1.41827744,  1.40163107])]
Total accumulated reward = 25.7617945166
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [3.2777739204016467, 8.8533853290219771, 14.578491488464124, 20.160188786681132, 25.761794516589966]
Normalized Reward history [1.9494370921021416, 6.1967116724229667, 10.593481003565611, 14.846841473483115, 19.120110375092445]
