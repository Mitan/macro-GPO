Physical State
[[ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]]
Measurements
[ 0.39174015  0.30209275  0.17453192  0.0021096  -0.20620779  0.0021096
  0.17453192  0.30209275  0.39174015  0.4801518   0.5861731   0.71524622
  0.83771261  0.92367505  0.93120079  0.82502671  0.61879287  0.82502671
  0.93120079  0.92367505  0.83771261]
===============================================Measurements Collected
[array([ 0.30209275,  0.17453192,  0.0021096 , -0.20620779]), array([ 0.0021096 ,  0.17453192,  0.30209275,  0.39174015]), array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261]), array([ 0.92367505,  0.93120079,  0.82502671,  0.61879287]), array([ 0.82502671,  0.93120079,  0.92367505,  0.83771261])]
Base measurements collected
[array([ 0.30209275,  0.17453192,  0.0021096 , -0.20620779]), array([ 0.0021096 ,  0.17453192,  0.30209275,  0.39174015]), array([ 0.4801518 ,  0.5861731 ,  0.71524622,  0.83771261]), array([ 0.92367505,  0.93120079,  0.82502671,  0.61879287]), array([ 0.82502671,  0.93120079,  0.92367505,  0.83771261])]
Total accumulated reward = 10.5785952355
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.27252647946158282, 1.1430009048919583, 3.7622846364279967, 7.0609800688622322, 10.578595235502988]
Normalized Reward history [-1.0558103488379222, -1.5136727517070518, -0.22272584847051835, 1.7476327556642119, 3.9369110940054619]
