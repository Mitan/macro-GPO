Physical State
[[ 0.55  0.6 ]
 [ 0.5   0.6 ]
 [ 0.45  0.6 ]
 [ 0.4   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   0.95]
 [ 0.8   0.9 ]
 [ 0.8   0.85]
 [ 0.8   0.8 ]
 [ 0.75  0.8 ]
 [ 0.7   0.8 ]
 [ 0.65  0.8 ]
 [ 0.6   0.8 ]
 [ 0.6   0.75]
 [ 0.6   0.7 ]
 [ 0.6   0.65]
 [ 0.6   0.6 ]
 [ 0.55  0.6 ]
 [ 0.5   0.6 ]
 [ 0.45  0.6 ]
 [ 0.4   0.6 ]]
Measurements
[ 0.03799932  0.19522726  0.34808706  0.50566102  0.6553123   0.68868993
  0.75137431  0.80577383  0.82435688  1.04187956  1.28101655  1.55434556
  1.81759882  1.73109198  1.54092181  1.27142687  0.94617414  0.99290097
  0.96625019  0.88304665  0.72096737]
===============================================Measurements Collected
[array([ 0.19522726,  0.34808706,  0.50566102,  0.6553123 ]), array([ 0.68868993,  0.75137431,  0.80577383,  0.82435688]), array([ 1.04187956,  1.28101655,  1.55434556,  1.81759882]), array([ 1.73109198,  1.54092181,  1.27142687,  0.94617414]), array([ 0.99290097,  0.96625019,  0.88304665,  0.72096737])]
Base measurements collected
[array([ 0.19522726,  0.34808706,  0.50566102,  0.6553123 ]), array([ 0.68868993,  0.75137431,  0.80577383,  0.82435688]), array([ 1.04187956,  1.28101655,  1.55434556,  1.81759882]), array([ 1.73109198,  1.54092181,  1.27142687,  0.94617414]), array([ 0.99290097,  0.96625019,  0.88304665,  0.72096737])]
Total accumulated reward = 19.522103073
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [1.7042876489001388, 4.7744826065294159, 10.469323090856651, 15.958937887576933, 19.522103072980578]
Normalized Reward history [1.4287738595020838, 4.2234550277333049, 9.6427817226624839, 14.856882729984711, 18.144534125990297]
