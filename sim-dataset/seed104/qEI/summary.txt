Physical State
[[ 1.15  1.6 ]
 [ 1.1   1.6 ]
 [ 1.05  1.6 ]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.05  1.4 ]
 [ 1.1   1.4 ]
 [ 1.15  1.4 ]
 [ 1.2   1.4 ]
 [ 1.2   1.45]
 [ 1.2   1.5 ]
 [ 1.2   1.55]
 [ 1.2   1.6 ]
 [ 1.15  1.6 ]
 [ 1.1   1.6 ]
 [ 1.05  1.6 ]
 [ 1.    1.6 ]]
Measurements
[ 0.03799932  0.07099141  0.19719536  0.38286907  0.58423098  0.75962378
  0.86995834  0.93839306  0.95207192  0.80242515  0.62811087  0.44406358
  0.29462725  0.13899105 -0.03361101 -0.21865025 -0.3902611  -0.0864976
  0.25475053  0.61207488  0.89927116]
===============================================Measurements Collected
[array([ 0.07099141,  0.19719536,  0.38286907,  0.58423098]), array([ 0.75962378,  0.86995834,  0.93839306,  0.95207192]), array([ 0.80242515,  0.62811087,  0.44406358,  0.29462725]), array([ 0.13899105, -0.03361101, -0.21865025, -0.3902611 ]), array([-0.0864976 ,  0.25475053,  0.61207488,  0.89927116])]
Base measurements collected
[array([ 0.07099141,  0.19719536,  0.38286907,  0.58423098]), array([ 0.75962378,  0.86995834,  0.93839306,  0.95207192]), array([ 0.80242515,  0.62811087,  0.44406358,  0.29462725]), array([ 0.13899105, -0.03361101, -0.21865025, -0.3902611 ]), array([-0.0864976 ,  0.25475053,  0.61207488,  0.89927116])]
Total accumulated reward = 8.10062842991
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [1.2352868205424929, 4.7553339196066799, 6.9245607716325015, 6.421029462387434, 8.1006284299075979]
Normalized Reward history [0.95977303114443802, 4.2043063408105699, 6.0980194034383359, 5.3189743047952138, 6.7230594829173231]
