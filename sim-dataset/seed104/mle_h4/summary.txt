Physical State
[[ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Measurements
[ 0.03799932  0.07111351  0.14961521  0.22052967  0.23355698  0.22052967
  0.14961521  0.07111351  0.03799932  0.07099141  0.19719536  0.38286907
  0.58423098  0.75962378  0.86995834  0.93839306  0.95207192  0.9408773
  0.93001392  0.92010654  0.89927116]
===============================================Measurements Collected
[array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698]), array([ 0.22052967,  0.14961521,  0.07111351,  0.03799932]), array([ 0.07099141,  0.19719536,  0.38286907,  0.58423098]), array([ 0.75962378,  0.86995834,  0.93839306,  0.95207192]), array([ 0.9408773 ,  0.93001392,  0.92010654,  0.89927116])]
Base measurements collected
[array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698]), array([ 0.22052967,  0.14961521,  0.07111351,  0.03799932]), array([ 0.07099141,  0.19719536,  0.38286907,  0.58423098]), array([ 0.75962378,  0.86995834,  0.93839306,  0.95207192]), array([ 0.9408773 ,  0.93001392,  0.92010654,  0.89927116])]
Total accumulated reward = 9.59967590981
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.67481536792065888, 1.1540730721946588, 2.3893598927371515, 5.9094069918013385, 9.5996759098065496]
Normalized Reward history [0.39930157852260395, 0.60304549339854896, 1.5628185245429869, 4.8073518342091184, 8.2221069628162748]
