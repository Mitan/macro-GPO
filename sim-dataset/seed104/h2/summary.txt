Physical State
[[ 1.15  1.6 ]
 [ 1.1   1.6 ]
 [ 1.05  1.6 ]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]
 [ 1.05  1.6 ]
 [ 1.1   1.6 ]
 [ 1.15  1.6 ]
 [ 1.2   1.6 ]
 [ 1.15  1.6 ]
 [ 1.1   1.6 ]
 [ 1.05  1.6 ]
 [ 1.    1.6 ]]
Measurements
[ 0.03799932  0.07099141  0.19719536  0.38286907  0.58423098  0.75962378
  0.86995834  0.93839306  0.95207192  0.9408773   0.93001392  0.92010654
  0.89927116  0.61207488  0.25475053 -0.0864976  -0.3902611  -0.0864976
  0.25475053  0.61207488  0.89927116]
===============================================Measurements Collected
[array([ 0.07099141,  0.19719536,  0.38286907,  0.58423098]), array([ 0.75962378,  0.86995834,  0.93839306,  0.95207192]), array([ 0.9408773 ,  0.93001392,  0.92010654,  0.89927116]), array([ 0.61207488,  0.25475053, -0.0864976 , -0.3902611 ]), array([-0.0864976 ,  0.25475053,  0.61207488,  0.89927116])]
Base measurements collected
[array([ 0.07099141,  0.19719536,  0.38286907,  0.58423098]), array([ 0.75962378,  0.86995834,  0.93839306,  0.95207192]), array([ 0.9408773 ,  0.93001392,  0.92010654,  0.89927116]), array([ 0.61207488,  0.25475053, -0.0864976 , -0.3902611 ]), array([-0.0864976 ,  0.25475053,  0.61207488,  0.89927116])]
Total accumulated reward = 10.5152685186
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [1.2352868205424929, 4.7553339196066799, 8.4456028376118901, 8.835669551070648, 10.515268518590812]
Normalized Reward history [0.95977303114443802, 4.2043063408105699, 7.6190614694177263, 7.7336143934784296, 9.1376995716005389]
