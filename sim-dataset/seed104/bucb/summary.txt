Physical State
[[ 0.6   0.95]
 [ 0.6   0.9 ]
 [ 0.6   0.85]
 [ 0.6   0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.85]
 [ 0.8   0.9 ]
 [ 0.8   0.95]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.6   0.95]
 [ 0.6   0.9 ]
 [ 0.6   0.85]
 [ 0.6   0.8 ]]
Measurements
[ 0.03799932  0.07111351  0.14961521  0.22052967  0.23355698  0.37314496
  0.50581656  0.6534423   0.82435688  0.80577383  0.75137431  0.68868993
  0.6553123   0.83802278  1.03722874  1.26359203  1.4940687   1.5900676
  1.69468529  1.79352282  1.81759882]
===============================================Measurements Collected
[array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698]), array([ 0.37314496,  0.50581656,  0.6534423 ,  0.82435688]), array([ 0.80577383,  0.75137431,  0.68868993,  0.6553123 ]), array([ 0.83802278,  1.03722874,  1.26359203,  1.4940687 ]), array([ 1.5900676 ,  1.69468529,  1.79352282,  1.81759882])]
Base measurements collected
[array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698]), array([ 0.37314496,  0.50581656,  0.6534423 ,  0.82435688]), array([ 0.80577383,  0.75137431,  0.68868993,  0.6553123 ]), array([ 0.83802278,  1.03722874,  1.26359203,  1.4940687 ]), array([ 1.5900676 ,  1.69468529,  1.79352282,  1.81759882])]
Total accumulated reward = 17.461513234
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.67481536792065888, 3.0315760674945733, 5.9327264470275685, 10.565638703269451, 17.461513233975143]
Normalized Reward history [0.39930157852260395, 2.4805484886984632, 5.1061850788334038, 9.4635835456772313, 16.083944286984867]
