Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 0.03799932  0.07111351  0.14961521  0.22052967  0.23355698  0.22052967
  0.14961521  0.07111351  0.03799932  0.07111351  0.14961521  0.22052967
  0.23355698  0.22052967  0.14961521  0.07111351  0.03799932  0.07111351
  0.14961521  0.22052967  0.23355698]
===============================================Measurements Collected
[array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698]), array([ 0.22052967,  0.14961521,  0.07111351,  0.03799932]), array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698]), array([ 0.22052967,  0.14961521,  0.07111351,  0.03799932]), array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698])]
Base measurements collected
[array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698]), array([ 0.22052967,  0.14961521,  0.07111351,  0.03799932]), array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698]), array([ 0.22052967,  0.14961521,  0.07111351,  0.03799932]), array([ 0.07111351,  0.14961521,  0.22052967,  0.23355698])]
Total accumulated reward = 2.98296151231
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.67481536792065888, 1.1540730721946588, 1.8288884401153176, 2.3081461443893176, 2.9829615123099766]
Normalized Reward history [0.39930157852260395, 0.60304549339854896, 1.0023470719211529, 1.2060909867970979, 1.6053925653197019]
