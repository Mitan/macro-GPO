Physical State
[[ 0.55  1.  ]
 [ 0.5   1.  ]
 [ 0.45  1.  ]
 [ 0.4   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.55  1.  ]
 [ 0.5   1.  ]
 [ 0.45  1.  ]
 [ 0.4   1.  ]
 [ 0.45  1.  ]
 [ 0.5   1.  ]
 [ 0.55  1.  ]
 [ 0.6   1.  ]
 [ 0.55  1.  ]
 [ 0.5   1.  ]
 [ 0.45  1.  ]
 [ 0.4   1.  ]]
Measurements
[ 0.03799932  0.19522726  0.34808706  0.50566102  0.6553123   0.83802278
  1.03722874  1.26359203  1.4940687   1.68237512  1.79822976  1.81209824
  1.7245426   1.81209824  1.79822976  1.68237512  1.4940687   1.68237512
  1.79822976  1.81209824  1.7245426 ]
===============================================Measurements Collected
[array([ 0.19522726,  0.34808706,  0.50566102,  0.6553123 ]), array([ 0.83802278,  1.03722874,  1.26359203,  1.4940687 ]), array([ 1.68237512,  1.79822976,  1.81209824,  1.7245426 ]), array([ 1.81209824,  1.79822976,  1.68237512,  1.4940687 ]), array([ 1.68237512,  1.79822976,  1.81209824,  1.7245426 ])]
Base measurements collected
[array([ 0.19522726,  0.34808706,  0.50566102,  0.6553123 ]), array([ 0.83802278,  1.03722874,  1.26359203,  1.4940687 ]), array([ 1.68237512,  1.79822976,  1.81209824,  1.7245426 ]), array([ 1.81209824,  1.79822976,  1.68237512,  1.4940687 ]), array([ 1.68237512,  1.79822976,  1.81209824,  1.7245426 ])]
Total accumulated reward = 27.1584631608
Nodes Expanded per stage
[118458, 111418, 22202, 6001, 5]
Total nodes expanded = 258084
Reward history [1.7042876489001388, 6.3371999051420218, 13.354445625134916, 20.14121744076386, 27.158463160756753]
Normalized Reward history [1.4287738595020838, 5.7861723263459126, 12.527904256940753, 19.039162283171642, 25.78089421376648]
