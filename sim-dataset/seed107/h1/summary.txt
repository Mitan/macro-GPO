Physical State
[[ 1.35  1.  ]
 [ 1.3   1.  ]
 [ 1.25  1.  ]
 [ 1.2   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.25  1.  ]
 [ 1.3   1.  ]
 [ 1.35  1.  ]
 [ 1.4   1.  ]
 [ 1.35  1.  ]
 [ 1.3   1.  ]
 [ 1.25  1.  ]
 [ 1.2   1.  ]]
Measurements
[-0.52585736 -0.49508097 -0.54734104 -0.6795217  -0.86557241 -0.6795217
 -0.54734104 -0.49508097 -0.52585736 -0.41239974 -0.27185502 -0.14562029
 -0.04249171 -0.0246573  -0.08828703 -0.20217937 -0.38293113 -0.20217937
 -0.08828703 -0.0246573  -0.04249171]
===============================================Measurements Collected
[array([-0.49508097, -0.54734104, -0.6795217 , -0.86557241]), array([-0.6795217 , -0.54734104, -0.49508097, -0.52585736]), array([-0.41239974, -0.27185502, -0.14562029, -0.04249171]), array([-0.0246573 , -0.08828703, -0.20217937, -0.38293113]), array([-0.20217937, -0.08828703, -0.0246573 , -0.04249171])]
Base measurements collected
[array([-0.49508097, -0.54734104, -0.6795217 , -0.86557241]), array([-0.6795217 , -0.54734104, -0.49508097, -0.52585736]), array([-0.41239974, -0.27185502, -0.14562029, -0.04249171]), array([-0.0246573 , -0.08828703, -0.20217937, -0.38293113]), array([-0.20217937, -0.08828703, -0.0246573 , -0.04249171])]
Total accumulated reward = -6.76335417181
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-2.5875161151378183, -4.8353171783101319, -5.7076839283339886, -6.4057387645224253, -6.7633541718110362]
Normalized Reward history [-2.3226180461418036, -4.3055210403181023, -4.9129897213459444, -5.3461464885383663, -5.4388638268309624]
