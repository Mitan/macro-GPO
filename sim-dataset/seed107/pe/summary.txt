Physical State
[[ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   0.95]
 [ 0.8   0.9 ]
 [ 0.8   0.85]
 [ 0.8   0.8 ]
 [ 0.85  0.8 ]
 [ 0.9   0.8 ]
 [ 0.95  0.8 ]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]]
Measurements
[-0.52585736 -0.5958773  -0.63893665 -0.6767862  -0.71922264 -0.67918158
 -0.67132217 -0.65857988 -0.61741369 -0.70094752 -0.81799839 -0.94269434
 -1.02727241 -1.00949893 -0.88811988 -0.70064763 -0.52791297 -0.45524501
 -0.34154551 -0.22377649 -0.10736634]
===============================================Measurements Collected
[array([-0.5958773 , -0.63893665, -0.6767862 , -0.71922264]), array([-0.67918158, -0.67132217, -0.65857988, -0.61741369]), array([-0.70094752, -0.81799839, -0.94269434, -1.02727241]), array([-1.00949893, -0.88811988, -0.70064763, -0.52791297]), array([-0.45524501, -0.34154551, -0.22377649, -0.10736634])]
Base measurements collected
[array([-0.5958773 , -0.63893665, -0.6767862 , -0.71922264]), array([-0.67918158, -0.67132217, -0.65857988, -0.61741369]), array([-0.70094752, -0.81799839, -0.94269434, -1.02727241]), array([-1.00949893, -0.88811988, -0.70064763, -0.52791297]), array([-0.45524501, -0.34154551, -0.22377649, -0.10736634])]
Total accumulated reward = -13.0003455241
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-2.6308227900306056, -5.2573201076149854, -8.7462327712068273, -11.872412183186011, -13.000345524073651]
Normalized Reward history [-2.3659247210345908, -4.7275239696229558, -7.9515385642187839, -10.812819907201954, -11.675855179093578]
