Physical State
[[ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   1.05]
 [ 0.8   1.1 ]
 [ 0.8   1.15]
 [ 0.8   1.2 ]
 [ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Measurements
[-0.52585736 -0.5958773  -0.63893665 -0.6767862  -0.71922264 -0.78731456
 -0.87479731 -0.95414961 -0.98326524 -1.02977613 -1.03270793 -0.98070543
 -0.86557241 -1.05274607 -1.17748727 -1.23882555 -1.22639937 -1.0902375
 -0.91014097 -0.71939075 -0.53373397]
===============================================Measurements Collected
[array([-0.5958773 , -0.63893665, -0.6767862 , -0.71922264]), array([-0.78731456, -0.87479731, -0.95414961, -0.98326524]), array([-1.02977613, -1.03270793, -0.98070543, -0.86557241]), array([-1.05274607, -1.17748727, -1.23882555, -1.22639937]), array([-1.0902375 , -0.91014097, -0.71939075, -0.53373397])]
Base measurements collected
[array([-0.5958773 , -0.63893665, -0.6767862 , -0.71922264]), array([-0.78731456, -0.87479731, -0.95414961, -0.98326524]), array([-1.02977613, -1.03270793, -0.98070543, -0.86557241]), array([-1.05274607, -1.17748727, -1.23882555, -1.22639937]), array([-1.0902375 , -0.91014097, -0.71939075, -0.53373397])]
Total accumulated reward = -18.0880728604
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-2.6308227900306056, -6.2303495020357245, -10.13911139862793, -14.834569665422716, -18.088072860379302]
Normalized Reward history [-2.3659247210345908, -5.7005533640436949, -9.3444171916398844, -13.774977389438655, -16.763582515399225]
