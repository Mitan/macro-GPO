Physical State
[[ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]]
Measurements
[-0.52585736 -0.5958773  -0.63893665 -0.6767862  -0.71922264 -0.6767862
 -0.63893665 -0.5958773  -0.52585736 -0.41239974 -0.27185502 -0.14562029
 -0.04249171 -0.21349514 -0.39985419 -0.56428997 -0.66183133 -0.56428997
 -0.39985419 -0.21349514 -0.04249171]
===============================================Measurements Collected
[array([-0.5958773 , -0.63893665, -0.6767862 , -0.71922264]), array([-0.6767862 , -0.63893665, -0.5958773 , -0.52585736]), array([-0.41239974, -0.27185502, -0.14562029, -0.04249171]), array([-0.21349514, -0.39985419, -0.56428997, -0.66183133]), array([-0.56428997, -0.39985419, -0.21349514, -0.04249171])]
Base measurements collected
[array([-0.5958773 , -0.63893665, -0.6767862 , -0.71922264]), array([-0.6767862 , -0.63893665, -0.5958773 , -0.52585736]), array([-0.41239974, -0.27185502, -0.14562029, -0.04249171]), array([-0.21349514, -0.39985419, -0.56428997, -0.66183133]), array([-0.56428997, -0.39985419, -0.21349514, -0.04249171])]
Total accumulated reward = -9.00024868471
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-2.6308227900306056, -5.068280301242635, -5.9406470512664917, -7.7801176811402888, -9.000248684711643]
Normalized Reward history [-2.3659247210345908, -4.5384841632506054, -5.1459528442784475, -6.7205254051562298, -7.6757583397315692]
