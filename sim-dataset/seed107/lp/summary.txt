Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[-0.52585736 -0.49508097 -0.54734104 -0.6795217  -0.86557241 -0.6795217
 -0.54734104 -0.49508097 -0.52585736 -0.64094087 -0.80543762 -0.9507713
 -1.02727241 -0.9507713  -0.80543762 -0.64094087 -0.52585736 -0.64094087
 -0.80543762 -0.9507713  -1.02727241]
===============================================Measurements Collected
[array([-0.49508097, -0.54734104, -0.6795217 , -0.86557241]), array([-0.6795217 , -0.54734104, -0.49508097, -0.52585736]), array([-0.64094087, -0.80543762, -0.9507713 , -1.02727241]), array([-0.9507713 , -0.80543762, -0.64094087, -0.52585736]), array([-0.64094087, -0.80543762, -0.9507713 , -1.02727241])]
Base measurements collected
[array([-0.49508097, -0.54734104, -0.6795217 , -0.86557241]), array([-0.6795217 , -0.54734104, -0.49508097, -0.52585736]), array([-0.64094087, -0.80543762, -0.9507713 , -1.02727241]), array([-0.9507713 , -0.80543762, -0.64094087, -0.52585736]), array([-0.64094087, -0.80543762, -0.9507713 , -1.02727241])]
Total accumulated reward = -14.6071687507
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-2.5875161151378183, -4.8353171783101319, -8.259739387967258, -11.182746541015909, -14.607168750673036]
Normalized Reward history [-2.3226180461418036, -4.3055210403181023, -7.4650451809792138, -10.12315426503185, -13.282678405692963]
