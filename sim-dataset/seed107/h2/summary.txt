Physical State
[[ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   0.95]
 [ 0.8   0.9 ]
 [ 0.8   0.85]
 [ 0.8   0.8 ]
 [ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]
 [ 0.85  0.6 ]
 [ 0.9   0.6 ]
 [ 0.95  0.6 ]
 [ 1.    0.6 ]
 [ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]]
Measurements
[-0.52585736 -0.5958773  -0.63893665 -0.6767862  -0.71922264 -0.67918158
 -0.67132217 -0.65857988 -0.61741369 -0.54111657 -0.40823393 -0.24553119
 -0.10736634 -0.22377649 -0.34154551 -0.45524501 -0.52791297 -0.45524501
 -0.34154551 -0.22377649 -0.10736634]
===============================================Measurements Collected
[array([-0.5958773 , -0.63893665, -0.6767862 , -0.71922264]), array([-0.67918158, -0.67132217, -0.65857988, -0.61741369]), array([-0.54111657, -0.40823393, -0.24553119, -0.10736634]), array([-0.22377649, -0.34154551, -0.45524501, -0.52791297]), array([-0.45524501, -0.34154551, -0.22377649, -0.10736634])]
Base measurements collected
[array([-0.5958773 , -0.63893665, -0.6767862 , -0.71922264]), array([-0.67918158, -0.67132217, -0.65857988, -0.61741369]), array([-0.54111657, -0.40823393, -0.24553119, -0.10736634]), array([-0.22377649, -0.34154551, -0.45524501, -0.52791297]), array([-0.45524501, -0.34154551, -0.22377649, -0.10736634])]
Total accumulated reward = -9.23598144662
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-2.6308227900306056, -5.2573201076149854, -6.5595681299271149, -8.1080481057297238, -9.2359814466173642]
Normalized Reward history [-2.3659247210345908, -4.7275239696229558, -5.7648739229390706, -7.0484558297456648, -7.9114911016372895]
