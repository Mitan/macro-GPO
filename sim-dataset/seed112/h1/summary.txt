Physical State
[[ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.75  1.2 ]
 [ 0.7   1.2 ]
 [ 0.65  1.2 ]
 [ 0.6   1.2 ]
 [ 0.65  1.2 ]
 [ 0.7   1.2 ]
 [ 0.75  1.2 ]
 [ 0.8   1.2 ]
 [ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]]
Measurements
[ 0.40832374  0.64091011  0.80644943  0.92635359  0.96283115  0.85791778
  0.75876055  0.6991208   0.69507886  0.73376552  0.7732233   0.78404108
  0.72070395  0.78404108  0.7732233   0.73376552  0.69507886  0.6991208
  0.75876055  0.85791778  0.96283115]
===============================================Measurements Collected
[array([ 0.64091011,  0.80644943,  0.92635359,  0.96283115]), array([ 0.85791778,  0.75876055,  0.6991208 ,  0.69507886]), array([ 0.73376552,  0.7732233 ,  0.78404108,  0.72070395]), array([ 0.78404108,  0.7732233 ,  0.73376552,  0.69507886]), array([ 0.6991208 ,  0.75876055,  0.85791778,  0.96283115])]
Base measurements collected
[array([ 0.64091011,  0.80644943,  0.92635359,  0.96283115]), array([ 0.85791778,  0.75876055,  0.6991208 ,  0.69507886]), array([ 0.73376552,  0.7732233 ,  0.78404108,  0.72070395]), array([ 0.78404108,  0.7732233 ,  0.73376552,  0.69507886]), array([ 0.6991208 ,  0.75876055,  0.85791778,  0.96283115])]
Total accumulated reward = 15.6238951608
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [3.336544294261639, 6.3474222818062724, 9.3591561248425279, 12.345264874435893, 15.623895160790489]
Normalized Reward history [4.5295813957318911, 8.7334964847467766, 12.938267429253283, 17.117413280316903, 21.589080668141754]
