Physical State
[[ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]
 [ 1.05  1.2 ]
 [ 1.1   1.2 ]
 [ 1.15  1.2 ]
 [ 1.2   1.2 ]
 [ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]]
Measurements
[ 0.40832374  0.64091011  0.80644943  0.92635359  0.96283115  0.85791778
  0.75876055  0.6991208   0.69507886  0.6991208   0.75876055  0.85791778
  0.96283115  1.01969924  0.98004916  0.81521325  0.52500819  0.81521325
  0.98004916  1.01969924  0.96283115]
===============================================Measurements Collected
[array([ 0.64091011,  0.80644943,  0.92635359,  0.96283115]), array([ 0.85791778,  0.75876055,  0.6991208 ,  0.69507886]), array([ 0.6991208 ,  0.75876055,  0.85791778,  0.96283115]), array([ 1.01969924,  0.98004916,  0.81521325,  0.52500819]), array([ 0.81521325,  0.98004916,  1.01969924,  0.96283115])]
Base measurements collected
[array([ 0.64091011,  0.80644943,  0.92635359,  0.96283115]), array([ 0.85791778,  0.75876055,  0.6991208 ,  0.69507886]), array([ 0.6991208 ,  0.75876055,  0.85791778,  0.96283115]), array([ 1.01969924,  0.98004916,  0.81521325,  0.52500819]), array([ 0.81521325,  0.98004916,  1.01969924,  0.96283115])]
Total accumulated reward = 16.7438152166
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [3.336544294261639, 6.3474222818062724, 9.6260525681608691, 12.966022409975169, 16.74381521664883]
Normalized Reward history [4.5295813957318911, 8.7334964847467766, 13.205163872571626, 17.738170815856179, 22.709000724000092]
