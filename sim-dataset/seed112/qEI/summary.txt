Physical State
[[ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]
 [ 1.2   1.05]
 [ 1.2   1.1 ]
 [ 1.2   1.15]
 [ 1.2   1.2 ]
 [ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]]
Measurements
[ 0.40832374  0.12995207 -0.17533903 -0.5010838  -0.79169806 -0.93227319
 -1.07770893 -1.21114466 -1.31221739 -1.19190295 -0.9965161  -0.75034745
 -0.45816042 -0.16128143  0.11719451  0.35417842  0.52500819  0.81521325
  0.98004916  1.01969924  0.96283115]
===============================================Measurements Collected
[array([ 0.12995207, -0.17533903, -0.5010838 , -0.79169806]), array([-0.93227319, -1.07770893, -1.21114466, -1.31221739]), array([-1.19190295, -0.9965161 , -0.75034745, -0.45816042]), array([-0.16128143,  0.11719451,  0.35417842,  0.52500819]), array([ 0.81521325,  0.98004916,  1.01969924,  0.96283115])]
Base measurements collected
[array([ 0.12995207, -0.17533903, -0.5010838 , -0.79169806]), array([-0.93227319, -1.07770893, -1.21114466, -1.31221739]), array([-1.19190295, -0.9965161 , -0.75034745, -0.45816042]), array([-0.16128143,  0.11719451,  0.35417842,  0.52500819]), array([ 0.81521325,  0.98004916,  1.01969924,  0.96283115])]
Total accumulated reward = -4.65554740969
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [-1.3381688172921118, -5.8715129945972491, -9.2684399135788489, -8.4333402163651279, -4.6555474096914677]
Normalized Reward history [-0.14513171582185969, -3.4854387916567449, -5.6893286091680935, -3.6611918104841203, 1.3096380976597919]
