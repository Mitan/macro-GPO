Physical State
[[ 1.05  1.4 ]
 [ 1.1   1.4 ]
 [ 1.15  1.4 ]
 [ 1.2   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   1.05]
 [ 0.8   1.1 ]
 [ 0.8   1.15]
 [ 0.8   1.2 ]
 [ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.05  1.4 ]
 [ 1.1   1.4 ]
 [ 1.15  1.4 ]
 [ 1.2   1.4 ]]
Measurements
[ 0.40832374  0.49807307  0.54337831  0.54382613  0.53062437  0.64759191
  0.71018978  0.72216447  0.69507886  0.6991208   0.75876055  0.85791778
  0.96283115  0.92136443  0.77701545  0.55053872  0.28029223  0.31986399
  0.34665806  0.34116084  0.27672182]
===============================================Measurements Collected
[array([ 0.49807307,  0.54337831,  0.54382613,  0.53062437]), array([ 0.64759191,  0.71018978,  0.72216447,  0.69507886]), array([ 0.6991208 ,  0.75876055,  0.85791778,  0.96283115]), array([ 0.92136443,  0.77701545,  0.55053872,  0.28029223]), array([ 0.31986399,  0.34665806,  0.34116084,  0.27672182])]
Base measurements collected
[array([ 0.49807307,  0.54337831,  0.54382613,  0.53062437]), array([ 0.64759191,  0.71018978,  0.72216447,  0.69507886]), array([ 0.6991208 ,  0.75876055,  0.85791778,  0.96283115]), array([ 0.92136443,  0.77701545,  0.55053872,  0.28029223]), array([ 0.31986399,  0.34665806,  0.34116084,  0.27672182])]
Total accumulated reward = 11.9831727107
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [2.1159018686003455, 4.8909268820015033, 8.1695571683560999, 10.698767996104603, 11.983172710688226]
Normalized Reward history [3.3089389700705976, 7.2770010849420075, 11.748668472766857, 15.470916401985612, 17.948358218039488]
