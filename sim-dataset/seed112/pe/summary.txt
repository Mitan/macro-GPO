Physical State
[[ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]]
Measurements
[ 0.40832374  0.12995207 -0.17533903 -0.5010838  -0.79169806 -0.93227319
 -1.07770893 -1.21114466 -1.31221739 -1.19190295 -0.9965161  -0.75034745
 -0.45816042 -0.18978168  0.06500125  0.26528972  0.40832374  0.64091011
  0.80644943  0.92635359  0.96283115]
===============================================Measurements Collected
[array([ 0.12995207, -0.17533903, -0.5010838 , -0.79169806]), array([-0.93227319, -1.07770893, -1.21114466, -1.31221739]), array([-1.19190295, -0.9965161 , -0.75034745, -0.45816042]), array([-0.18978168,  0.06500125,  0.26528972,  0.40832374]), array([ 0.64091011,  0.80644943,  0.92635359,  0.96283115])]
Base measurements collected
[array([ 0.12995207, -0.17533903, -0.5010838 , -0.79169806]), array([-0.93227319, -1.07770893, -1.21114466, -1.31221739]), array([-1.19190295, -0.9965161 , -0.75034745, -0.45816042]), array([-0.18978168,  0.06500125,  0.26528972,  0.40832374]), array([ 0.64091011,  0.80644943,  0.92635359,  0.96283115])]
Total accumulated reward = -5.3830625903
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-1.3381688172921118, -5.8715129945972491, -9.2684399135788489, -8.7196068845607453, -5.3830625902991063]
Normalized Reward history [-0.14513171582185969, -3.4854387916567449, -5.6893286091680935, -3.9474584786797369, 0.58212291705215424]
