Physical State
[[ 0.8   0.85]
 [ 0.8   0.9 ]
 [ 0.8   0.95]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]
 [ 0.8   0.95]
 [ 0.8   0.9 ]
 [ 0.8   0.85]
 [ 0.8   0.8 ]
 [ 0.8   0.85]
 [ 0.8   0.9 ]
 [ 0.8   0.95]
 [ 0.8   1.  ]]
Measurements
[ 0.40832374  0.49807307  0.54337831  0.54382613  0.53062437  0.49306102
  0.43816674  0.34183598  0.18954361  0.34183598  0.43816674  0.49306102
  0.53062437  0.33696635  0.06467438 -0.25719337 -0.57508561 -0.25719337
  0.06467438  0.33696635  0.53062437]
===============================================Measurements Collected
[array([ 0.49807307,  0.54337831,  0.54382613,  0.53062437]), array([ 0.49306102,  0.43816674,  0.34183598,  0.18954361]), array([ 0.34183598,  0.43816674,  0.49306102,  0.53062437]), array([ 0.33696635,  0.06467438, -0.25719337, -0.57508561]), array([-0.25719337,  0.06467438,  0.33696635,  0.53062437])]
Base measurements collected
[array([ 0.49807307,  0.54337831,  0.54382613,  0.53062437]), array([ 0.49306102,  0.43816674,  0.34183598,  0.18954361]), array([ 0.34183598,  0.43816674,  0.49306102,  0.53062437]), array([ 0.33696635,  0.06467438, -0.25719337, -0.57508561]), array([-0.25719337,  0.06467438,  0.33696635,  0.53062437])]
Total accumulated reward = 5.62663078104
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [2.1159018686003455, 3.5785092115130075, 5.3821973126519724, 4.9515590608644011, 5.6266307810437528]
Normalized Reward history [3.3089389700705976, 5.9645834144535117, 8.9613086170627287, 9.7237074667454095, 11.591816288395012]
