Physical State
[[ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]]
Measurements
[ 0.40832374  0.49807307  0.54337831  0.54382613  0.53062437  0.49306102
  0.43816674  0.34183598  0.18954361  0.34183598  0.43816674  0.49306102
  0.53062437  0.49306102  0.43816674  0.34183598  0.18954361  0.34183598
  0.43816674  0.49306102  0.53062437]
===============================================Measurements Collected
[array([ 0.49807307,  0.54337831,  0.54382613,  0.53062437]), array([ 0.49306102,  0.43816674,  0.34183598,  0.18954361]), array([ 0.34183598,  0.43816674,  0.49306102,  0.53062437]), array([ 0.49306102,  0.43816674,  0.34183598,  0.18954361]), array([ 0.34183598,  0.43816674,  0.49306102,  0.53062437])]
Base measurements collected
[array([ 0.49807307,  0.54337831,  0.54382613,  0.53062437]), array([ 0.49306102,  0.43816674,  0.34183598,  0.18954361]), array([ 0.34183598,  0.43816674,  0.49306102,  0.53062437]), array([ 0.49306102,  0.43816674,  0.34183598,  0.18954361]), array([ 0.34183598,  0.43816674,  0.49306102,  0.53062437])]
Total accumulated reward = 8.6484927567
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [2.1159018686003455, 3.5785092115130075, 5.3821973126519724, 6.8448046555646345, 8.6484927567036003]
Normalized Reward history [3.3089389700705976, 5.9645834144535117, 8.9613086170627287, 11.616953061445642, 14.613678264054858]
