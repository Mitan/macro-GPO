Physical State
[[ 0.75  1.2 ]
 [ 0.7   1.2 ]
 [ 0.65  1.2 ]
 [ 0.6   1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.75  1.2 ]
 [ 0.7   1.2 ]
 [ 0.65  1.2 ]
 [ 0.6   1.2 ]]
Measurements
[ 0.40832374  0.26528972  0.06500125 -0.18978168 -0.45816042 -0.18978168
  0.06500125  0.26528972  0.40832374  0.64091011  0.80644943  0.92635359
  0.96283115  0.85791778  0.75876055  0.6991208   0.69507886  0.73376552
  0.7732233   0.78404108  0.72070395]
===============================================Measurements Collected
[array([ 0.26528972,  0.06500125, -0.18978168, -0.45816042]), array([-0.18978168,  0.06500125,  0.26528972,  0.40832374]), array([ 0.64091011,  0.80644943,  0.92635359,  0.96283115]), array([ 0.85791778,  0.75876055,  0.6991208 ,  0.69507886]), array([ 0.73376552,  0.7732233 ,  0.78404108,  0.72070395])]
Base measurements collected
[array([ 0.26528972,  0.06500125, -0.18978168, -0.45816042]), array([-0.18978168,  0.06500125,  0.26528972,  0.40832374]), array([ 0.64091011,  0.80644943,  0.92635359,  0.96283115]), array([ 0.85791778,  0.75876055,  0.6991208 ,  0.69507886]), array([ 0.73376552,  0.7732233 ,  0.78404108,  0.72070395])]
Total accumulated reward = 9.59033802647
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-0.31765112738904722, 0.23118190162905705, 3.5677261958906961, 6.5786041834353295, 9.590338026471585]
Normalized Reward history [0.87538597408120489, 2.6172561045695613, 7.1468375003014524, 11.350752589316336, 15.555523533822843]
