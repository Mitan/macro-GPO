Physical State
[[ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.8   1.25]
 [ 0.8   1.3 ]
 [ 0.8   1.35]
 [ 0.8   1.4 ]
 [ 0.85  1.4 ]
 [ 0.9   1.4 ]
 [ 0.95  1.4 ]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Measurements
[-0.69234348 -0.33168191 -0.01520321  0.19777339  0.27016341  0.16562752
  0.00982765 -0.15887076 -0.31489522 -0.48197774 -0.67957744 -0.88328413
 -1.04420808 -0.97680289 -0.84925956 -0.67640187 -0.48367605 -0.2051489
  0.04118877  0.21747577  0.27016341]
===============================================Measurements Collected
[array([-0.33168191, -0.01520321,  0.19777339,  0.27016341]), array([ 0.16562752,  0.00982765, -0.15887076, -0.31489522]), array([-0.48197774, -0.67957744, -0.88328413, -1.04420808]), array([-0.97680289, -0.84925956, -0.67640187, -0.48367605]), array([-0.2051489 ,  0.04118877,  0.21747577,  0.27016341])]
Base measurements collected
[array([-0.33168191, -0.01520321,  0.19777339,  0.27016341]), array([ 0.16562752,  0.00982765, -0.15887076, -0.31489522]), array([-0.48197774, -0.67957744, -0.88328413, -1.04420808]), array([-0.97680289, -0.84925956, -0.67640187, -0.48367605]), array([-0.2051489 ,  0.04118877,  0.21747577,  0.27016341])]
Total accumulated reward = -5.92876784181
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.12105168174810671, -0.17725912632910329, -3.2663065184094875, -6.2524468885533118, -5.9287678418063425]
Normalized Reward history [1.4385433346640217, 2.4577241795027271, 0.68616844033825819, -0.98248027688965056, 0.65869042277323397]
