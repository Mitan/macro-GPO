Physical State
[[ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]]
Measurements
[-0.69234348 -1.03283308 -1.28718118 -1.43217451 -1.44687512 -1.43217451
 -1.28718118 -1.03283308 -0.69234348 -0.33168191 -0.01520321  0.19777339
  0.27016341  0.16562752  0.00982765 -0.15887076 -0.31489522 -0.15887076
  0.00982765  0.16562752  0.27016341]
===============================================Measurements Collected
[array([-1.03283308, -1.28718118, -1.43217451, -1.44687512]), array([-1.43217451, -1.28718118, -1.03283308, -0.69234348]), array([-0.33168191, -0.01520321,  0.19777339,  0.27016341]), array([ 0.16562752,  0.00982765, -0.15887076, -0.31489522]), array([-0.15887076,  0.00982765,  0.16562752,  0.27016341])]
Base measurements collected
[array([-1.03283308, -1.28718118, -1.43217451, -1.44687512]), array([-1.43217451, -1.28718118, -1.03283308, -0.69234348]), array([-0.33168191, -0.01520321,  0.19777339,  0.27016341]), array([ 0.16562752,  0.00982765, -0.15887076, -0.31489522]), array([-0.15887076,  0.00982765,  0.16562752,  0.27016341])]
Total accumulated reward = -9.53410745343
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-5.1990638951377264, -9.6435961519760376, -9.5225444702279312, -9.8208552783051406, -9.5341074534271151]
Normalized Reward history [-3.8815722422218113, -7.0086128461442065, -5.5700695114801846, -4.5508886666414794, -2.9466491888475383]
