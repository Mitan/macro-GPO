Physical State
[[ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.75]
 [ 1.2   0.7 ]
 [ 1.2   0.65]
 [ 1.2   0.6 ]
 [ 1.15  0.6 ]
 [ 1.1   0.6 ]
 [ 1.05  0.6 ]
 [ 1.    0.6 ]
 [ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]]
Measurements
[-0.69234348 -1.03283308 -1.28718118 -1.43217451 -1.44687512 -1.72153179
 -2.00944416 -2.2790799  -2.48290951 -2.47847623 -2.39388028 -2.28146815
 -2.17965829 -1.85139418 -1.55038413 -1.32645664 -1.20251362 -1.20695428
 -1.29215602 -1.45135131 -1.64661932]
===============================================Measurements Collected
[array([-1.03283308, -1.28718118, -1.43217451, -1.44687512]), array([-1.72153179, -2.00944416, -2.2790799 , -2.48290951]), array([-2.47847623, -2.39388028, -2.28146815, -2.17965829]), array([-1.85139418, -1.55038413, -1.32645664, -1.20251362]), array([-1.20695428, -1.29215602, -1.45135131, -1.64661932])]
Base measurements collected
[array([-1.03283308, -1.28718118, -1.43217451, -1.44687512]), array([-1.72153179, -2.00944416, -2.2790799 , -2.48290951]), array([-2.47847623, -2.39388028, -2.28146815, -2.17965829]), array([-1.85139418, -1.55038413, -1.32645664, -1.20251362]), array([-1.20695428, -1.29215602, -1.45135131, -1.64661932])]
Total accumulated reward = -34.553341729
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-5.1990638951377264, -13.692029258972262, -23.025512211176334, -28.956260794338927, -34.553341728980563]
Normalized Reward history [-3.8815722422218113, -11.057045953140431, -19.073037252428591, -23.686294182675269, -27.965883464400989]
