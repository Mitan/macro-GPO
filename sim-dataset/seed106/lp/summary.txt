Physical State
[[ 1.    0.25]
 [ 1.    0.3 ]
 [ 1.    0.35]
 [ 1.    0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.55]
 [ 1.    0.5 ]
 [ 1.    0.45]
 [ 1.    0.4 ]
 [ 1.    0.35]
 [ 1.    0.3 ]
 [ 1.    0.25]
 [ 1.    0.2 ]
 [ 1.    0.25]
 [ 1.    0.3 ]
 [ 1.    0.35]
 [ 1.    0.4 ]]
Measurements
[-0.69234348 -1.03283308 -1.28718118 -1.43217451 -1.44687512 -1.38125397
 -1.2907118  -1.22286384 -1.20251362 -1.24446623 -1.29587562 -1.32068755
 -1.28082498 -1.15171016 -0.95033993 -0.67127133 -0.3653292  -0.67127133
 -0.95033993 -1.15171016 -1.28082498]
===============================================Measurements Collected
[array([-1.03283308, -1.28718118, -1.43217451, -1.44687512]), array([-1.38125397, -1.2907118 , -1.22286384, -1.20251362]), array([-1.24446623, -1.29587562, -1.32068755, -1.28082498]), array([-1.15171016, -0.95033993, -0.67127133, -0.3653292 ]), array([-0.67127133, -0.95033993, -1.15171016, -1.28082498])]
Base measurements collected
[array([-1.03283308, -1.28718118, -1.43217451, -1.44687512]), array([-1.38125397, -1.2907118 , -1.22286384, -1.20251362]), array([-1.24446623, -1.29587562, -1.32068755, -1.28082498]), array([-1.15171016, -0.95033993, -0.67127133, -0.3653292 ]), array([-0.67127133, -0.95033993, -1.15171016, -1.28082498])]
Total accumulated reward = -22.6310585436
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-5.1990638951377264, -10.296407136083825, -15.438261514358596, -18.576912140787886, -22.631058543551795]
Normalized Reward history [-3.8815722422218113, -7.6614238302519952, -11.485786555610851, -13.306945529124226, -16.043600278972221]
