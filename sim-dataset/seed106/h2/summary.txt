Physical State
[[ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.05  1.2 ]
 [ 1.1   1.2 ]
 [ 1.15  1.2 ]
 [ 1.2   1.2 ]
 [ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]
 [ 1.05  1.2 ]
 [ 1.1   1.2 ]
 [ 1.15  1.2 ]
 [ 1.2   1.2 ]
 [ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]]
Measurements
[-0.69234348 -0.33168191 -0.01520321  0.19777339  0.27016341  0.32200653
  0.28693971  0.1619732  -0.0366336   0.1619732   0.28693971  0.32200653
  0.27016341  0.32200653  0.28693971  0.1619732  -0.0366336   0.1619732
  0.28693971  0.32200653  0.27016341]
===============================================Measurements Collected
[array([-0.33168191, -0.01520321,  0.19777339,  0.27016341]), array([ 0.32200653,  0.28693971,  0.1619732 , -0.0366336 ]), array([ 0.1619732 ,  0.28693971,  0.32200653,  0.27016341]), array([ 0.32200653,  0.28693971,  0.1619732 , -0.0366336 ]), array([ 0.1619732 ,  0.28693971,  0.32200653,  0.27016341])]
Base measurements collected
[array([-0.33168191, -0.01520321,  0.19777339,  0.27016341]), array([ 0.32200653,  0.28693971,  0.1619732 , -0.0366336 ]), array([ 0.1619732 ,  0.28693971,  0.32200653,  0.27016341]), array([ 0.32200653,  0.28693971,  0.1619732 , -0.0366336 ]), array([ 0.1619732 ,  0.28693971,  0.32200653,  0.27016341])]
Total accumulated reward = 3.67178905107
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.12105168174810671, 0.85533752144139941, 1.8964203664098835, 2.6307062061031763, 3.6717890510716602]
Normalized Reward history [1.4385433346640217, 3.49032082727323, 5.8488953251576294, 7.9006728177668375, 10.259247315651237]
