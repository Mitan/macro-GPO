Physical State
[[ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.85  1.2 ]
 [ 0.9   1.2 ]
 [ 0.95  1.2 ]
 [ 1.    1.2 ]
 [ 1.05  1.2 ]
 [ 1.1   1.2 ]
 [ 1.15  1.2 ]
 [ 1.2   1.2 ]
 [ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]]
Measurements
[-0.69234348 -0.33168191 -0.01520321  0.19777339  0.27016341  0.16562752
  0.00982765 -0.15887076 -0.31489522 -0.15887076  0.00982765  0.16562752
  0.27016341  0.32200653  0.28693971  0.1619732  -0.0366336   0.1619732
  0.28693971  0.32200653  0.27016341]
===============================================Measurements Collected
[array([-0.33168191, -0.01520321,  0.19777339,  0.27016341]), array([ 0.16562752,  0.00982765, -0.15887076, -0.31489522]), array([-0.15887076,  0.00982765,  0.16562752,  0.27016341]), array([ 0.32200653,  0.28693971,  0.1619732 , -0.0366336 ]), array([ 0.1619732 ,  0.28693971,  0.32200653,  0.27016341])]
Base measurements collected
[array([-0.33168191, -0.01520321,  0.19777339,  0.27016341]), array([ 0.16562752,  0.00982765, -0.15887076, -0.31489522]), array([-0.15887076,  0.00982765,  0.16562752,  0.27016341]), array([ 0.32200653,  0.28693971,  0.1619732 , -0.0366336 ]), array([ 0.1619732 ,  0.28693971,  0.32200653,  0.27016341])]
Total accumulated reward = 1.88485738321
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.12105168174810671, -0.17725912632910329, 0.10948869854892257, 0.84377453824221527, 1.8848573832106994]
Normalized Reward history [1.4385433346640217, 2.4577241795027271, 4.0619636572966682, 6.1137411499058762, 8.4723156477902748]
