Physical State
[[ 0.6   1.05]
 [ 0.6   1.1 ]
 [ 0.6   1.15]
 [ 0.6   1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.85]
 [ 0.8   0.9 ]
 [ 0.8   0.95]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.6   1.05]
 [ 0.6   1.1 ]
 [ 0.6   1.15]
 [ 0.6   1.2 ]]
Measurements
[-0.69234348 -1.03283308 -1.28718118 -1.43217451 -1.44687512 -1.21230191
 -1.05141657 -0.99671049 -1.03444497 -0.87558749 -0.71609303 -0.53960115
 -0.37796566 -0.4596351  -0.60177096 -0.76564921 -0.92460956 -0.78439285
 -0.70442968 -0.6681075  -0.68582723]
===============================================Measurements Collected
[array([-1.03283308, -1.28718118, -1.43217451, -1.44687512]), array([-1.21230191, -1.05141657, -0.99671049, -1.03444497]), array([-0.87558749, -0.71609303, -0.53960115, -0.37796566]), array([-0.4596351 , -0.60177096, -0.76564921, -0.92460956]), array([-0.78439285, -0.70442968, -0.6681075 , -0.68582723])]
Base measurements collected
[array([-1.03283308, -1.28718118, -1.43217451, -1.44687512]), array([-1.21230191, -1.05141657, -0.99671049, -1.03444497]), array([-0.87558749, -0.71609303, -0.53960115, -0.37796566]), array([-0.4596351 , -0.60177096, -0.76564921, -0.92460956]), array([-0.78439285, -0.70442968, -0.6681075 , -0.68582723])]
Total accumulated reward = -17.5976072571
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [-5.1990638951377264, -9.4939378285879599, -12.003185162840627, -14.754849991431703, -17.59760725710218]
Normalized Reward history [-3.8815722422218113, -6.8589545227561297, -8.0507102040928817, -9.4848833797680427, -11.010148992522605]
