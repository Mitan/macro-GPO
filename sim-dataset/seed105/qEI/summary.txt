Physical State
[[ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   1.05]
 [ 0.8   1.1 ]
 [ 0.8   1.15]
 [ 0.8   1.2 ]
 [ 0.8   1.25]
 [ 0.8   1.3 ]
 [ 0.8   1.35]
 [ 0.8   1.4 ]
 [ 0.85  1.4 ]
 [ 0.9   1.4 ]
 [ 0.95  1.4 ]
 [ 1.    1.4 ]
 [ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Measurements
[ 0.5656135   0.45684258  0.27759102  0.05644317 -0.17547617 -0.20152404
 -0.05802584  0.22675652  0.58481823  0.95132495  1.25141661  1.4518258
  1.5163855   1.36749108  1.17272066  0.96796566  0.74225775  0.96796566
  1.17272066  1.36749108  1.5163855 ]
===============================================Measurements Collected
[array([ 0.45684258,  0.27759102,  0.05644317, -0.17547617]), array([-0.20152404, -0.05802584,  0.22675652,  0.58481823]), array([ 0.95132495,  1.25141661,  1.4518258 ,  1.5163855 ]), array([ 1.36749108,  1.17272066,  0.96796566,  0.74225775]), array([ 0.96796566,  1.17272066,  1.36749108,  1.5163855 ])]
Base measurements collected
[array([ 0.45684258,  0.27759102,  0.05644317, -0.17547617]), array([-0.20152404, -0.05802584,  0.22675652,  0.58481823]), array([ 0.95132495,  1.25141661,  1.4518258 ,  1.5163855 ]), array([ 1.36749108,  1.17272066,  0.96796566,  0.74225775]), array([ 0.96796566,  1.17272066,  1.36749108,  1.5163855 ])]
Total accumulated reward = 15.6133763741
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [0.61540059137906256, 1.1674254680570053, 6.3383783331627574, 10.588813480645017, 15.613376374055473]
Normalized Reward history [0.83884317040199918, 1.6143106261028786, 7.0087060702315673, 11.482583796736764, 16.730589269170157]
