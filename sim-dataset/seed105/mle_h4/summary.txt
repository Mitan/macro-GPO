Physical State
[[ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]]
Measurements
[ 0.5656135   0.33646805  0.21674164  0.20529902  0.29405401  0.20529902
  0.21674164  0.33646805  0.5656135   0.86230251  1.17554985  1.44675119
  1.64829333  1.77247553  1.79762024  1.73356679  1.58284274  1.73356679
  1.79762024  1.77247553  1.64829333]
===============================================Measurements Collected
[array([ 0.33646805,  0.21674164,  0.20529902,  0.29405401]), array([ 0.20529902,  0.21674164,  0.33646805,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.77247553,  1.79762024,  1.73356679,  1.58284274]), array([ 1.73356679,  1.79762024,  1.77247553,  1.64829333])]
Base measurements collected
[array([ 0.33646805,  0.21674164,  0.20529902,  0.29405401]), array([ 0.20529902,  0.21674164,  0.33646805,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.77247553,  1.79762024,  1.73356679,  1.58284274]), array([ 1.73356679,  1.79762024,  1.77247553,  1.64829333])]
Total accumulated reward = 21.3480429868
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [1.0525627046147654, 2.3766849049240459, 7.5095817735021937, 14.396087088370066, 21.34804298680832]
Normalized Reward history [1.276005283637702, 2.8235700629699192, 8.1799095105710045, 15.289857404461815, 22.465255881923003]
