Physical State
[[ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]]
Measurements
[ 0.5656135   0.86230251  1.17554985  1.44675119  1.64829333  1.77247553
  1.79762024  1.73356679  1.58284274  1.73356679  1.79762024  1.77247553
  1.64829333  1.77247553  1.79762024  1.73356679  1.58284274  1.73356679
  1.79762024  1.77247553  1.64829333]
===============================================Measurements Collected
[array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.77247553,  1.79762024,  1.73356679,  1.58284274]), array([ 1.73356679,  1.79762024,  1.77247553,  1.64829333]), array([ 1.77247553,  1.79762024,  1.73356679,  1.58284274]), array([ 1.73356679,  1.79762024,  1.77247553,  1.64829333])]
Base measurements collected
[array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.77247553,  1.79762024,  1.73356679,  1.58284274]), array([ 1.73356679,  1.79762024,  1.77247553,  1.64829333]), array([ 1.77247553,  1.79762024,  1.73356679,  1.58284274]), array([ 1.73356679,  1.79762024,  1.77247553,  1.64829333])]
Total accumulated reward = 32.8098192952
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [5.1328968685781478, 12.019402183446022, 18.971358081884276, 25.857863396752151, 32.809819295190401]
Normalized Reward history [5.3563394476010844, 12.466287341491896, 19.641685818953086, 26.751633712843898, 33.927032190305084]
