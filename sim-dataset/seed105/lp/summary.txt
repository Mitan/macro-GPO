Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 0.5656135   0.86230251  1.17554985  1.44675119  1.64829333  1.44675119
  1.17554985  0.86230251  0.5656135   0.86230251  1.17554985  1.44675119
  1.64829333  1.44675119  1.17554985  0.86230251  0.5656135   0.86230251
  1.17554985  1.44675119  1.64829333]
===============================================Measurements Collected
[array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.44675119,  1.17554985,  0.86230251,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.44675119,  1.17554985,  0.86230251,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333])]
Base measurements collected
[array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.44675119,  1.17554985,  0.86230251,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.44675119,  1.17554985,  0.86230251,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333])]
Total accumulated reward = 23.49912469
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [5.1328968685781478, 9.1831139107305084, 14.316010779308655, 18.366227821461017, 23.499124690039164]
Normalized Reward history [5.3563394476010844, 9.6299990687763817, 14.986338516377465, 19.259998137552763, 24.616337585153847]
