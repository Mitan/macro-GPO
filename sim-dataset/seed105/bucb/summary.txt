Physical State
[[ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.05  1.2 ]
 [ 1.1   1.2 ]
 [ 1.15  1.2 ]
 [ 1.2   1.2 ]
 [ 1.2   1.25]
 [ 1.2   1.3 ]
 [ 1.2   1.35]
 [ 1.2   1.4 ]
 [ 1.15  1.4 ]
 [ 1.1   1.4 ]
 [ 1.05  1.4 ]
 [ 1.    1.4 ]
 [ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Measurements
[ 0.5656135   0.33646805  0.21674164  0.20529902  0.29405401  0.05258012
 -0.25692571 -0.62704252 -1.03678513 -0.9130503  -0.67768681 -0.38238808
 -0.10138728  0.08856181  0.29615744  0.51673979  0.74225775  0.96796566
  1.17272066  1.36749108  1.5163855 ]
===============================================Measurements Collected
[array([ 0.33646805,  0.21674164,  0.20529902,  0.29405401]), array([ 0.05258012, -0.25692571, -0.62704252, -1.03678513]), array([-0.9130503 , -0.67768681, -0.38238808, -0.10138728]), array([ 0.08856181,  0.29615744,  0.51673979,  0.74225775]), array([ 0.96796566,  1.17272066,  1.36749108,  1.5163855 ])]
Base measurements collected
[array([ 0.33646805,  0.21674164,  0.20529902,  0.29405401]), array([ 0.05258012, -0.25692571, -0.62704252, -1.03678513]), array([-0.9130503 , -0.67768681, -0.38238808, -0.10138728]), array([ 0.08856181,  0.29615744,  0.51673979,  0.74225775]), array([ 0.96796566,  1.17272066,  1.36749108,  1.5163855 ])]
Total accumulated reward = 3.77815667931
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [1.0525627046147654, -0.81561054118536891, -2.8901230001664655, -1.2464062140970813, 3.778156679313375]
Normalized Reward history [1.276005283637702, -0.36872538313949565, -2.2197952630976556, -0.35263589800533479, 4.8953695744280576]
