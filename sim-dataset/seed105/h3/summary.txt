Physical State
[[ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.65]
 [ 1.    0.7 ]
 [ 1.    0.75]
 [ 1.    0.8 ]]
Measurements
[ 0.5656135   0.57628182  0.4547083   0.22724395 -0.07138503  0.22724395
  0.4547083   0.57628182  0.5656135   0.86230251  1.17554985  1.44675119
  1.64829333  1.77247553  1.79762024  1.73356679  1.58284274  1.73356679
  1.79762024  1.77247553  1.64829333]
===============================================Measurements Collected
[array([ 0.57628182,  0.4547083 ,  0.22724395, -0.07138503]), array([ 0.22724395,  0.4547083 ,  0.57628182,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.77247553,  1.79762024,  1.73356679,  1.58284274]), array([ 1.73356679,  1.79762024,  1.77247553,  1.64829333])]
Base measurements collected
[array([ 0.57628182,  0.4547083 ,  0.22724395, -0.07138503]), array([ 0.22724395,  0.4547083 ,  0.57628182,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.77247553,  1.79762024,  1.73356679,  1.58284274]), array([ 1.73356679,  1.79762024,  1.77247553,  1.64829333])]
Total accumulated reward = 21.9820546922
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [1.1868490389989077, 3.0106966103077539, 8.1435934788859008, 15.030098793753774, 21.982054692192026]
Normalized Reward history [1.4102916180218443, 3.4575817683536272, 8.8139212159547107, 15.923869109845521, 23.099267587306709]
