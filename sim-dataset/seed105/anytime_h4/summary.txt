Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 0.5656135   0.33646805  0.21674164  0.20529902  0.29405401  0.4355008
  0.60426926  0.70989899  0.74225775  0.70989899  0.60426926  0.4355008
  0.29405401  0.20529902  0.21674164  0.33646805  0.5656135   0.86230251
  1.17554985  1.44675119  1.64829333]
===============================================Measurements Collected
[array([ 0.33646805,  0.21674164,  0.20529902,  0.29405401]), array([ 0.4355008 ,  0.60426926,  0.70989899,  0.74225775]), array([ 0.70989899,  0.60426926,  0.4355008 ,  0.29405401]), array([ 0.20529902,  0.21674164,  0.33646805,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333])]
Base measurements collected
[array([ 0.33646805,  0.21674164,  0.20529902,  0.29405401]), array([ 0.4355008 ,  0.60426926,  0.70989899,  0.74225775]), array([ 0.70989899,  0.60426926,  0.4355008 ,  0.29405401]), array([ 0.20529902,  0.21674164,  0.33646805,  0.5656135 ]), array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333])]
Total accumulated reward = 12.04523164
Nodes Expanded per stage
[118458, 107834, 22714, 6001, 5]
Total nodes expanded = 255012
Reward history [1.0525627046147654, 3.5444895123475089, 5.5882125711486808, 6.9123347714579619, 12.045231640036111]
Normalized Reward history [1.276005283637702, 3.9913746703933821, 6.2585403082174906, 7.8061050875497084, 13.162444535150794]
