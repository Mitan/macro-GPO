Physical State
[[ 0.65  1.4 ]
 [ 0.7   1.4 ]
 [ 0.75  1.4 ]
 [ 0.8   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]
 [ 0.75  1.4 ]
 [ 0.7   1.4 ]
 [ 0.65  1.4 ]
 [ 0.6   1.4 ]
 [ 0.65  1.4 ]
 [ 0.7   1.4 ]
 [ 0.75  1.4 ]
 [ 0.8   1.4 ]]
Measurements
[ 0.5656135   0.33646805  0.21674164  0.20529902  0.29405401  0.4355008
  0.60426926  0.70989899  0.74225775  0.96796566  1.17272066  1.36749108
  1.5163855   1.63496539  1.69699595  1.70282601  1.63451755  1.70282601
  1.69699595  1.63496539  1.5163855 ]
===============================================Measurements Collected
[array([ 0.33646805,  0.21674164,  0.20529902,  0.29405401]), array([ 0.4355008 ,  0.60426926,  0.70989899,  0.74225775]), array([ 0.96796566,  1.17272066,  1.36749108,  1.5163855 ]), array([ 1.63496539,  1.69699595,  1.70282601,  1.63451755]), array([ 1.70282601,  1.69699595,  1.63496539,  1.5163855 ])]
Base measurements collected
[array([ 0.33646805,  0.21674164,  0.20529902,  0.29405401]), array([ 0.4355008 ,  0.60426926,  0.70989899,  0.74225775]), array([ 0.96796566,  1.17272066,  1.36749108,  1.5163855 ]), array([ 1.63496539,  1.69699595,  1.70282601,  1.63451755]), array([ 1.70282601,  1.69699595,  1.63496539,  1.5163855 ])]
Total accumulated reward = 21.7895301551
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [1.0525627046147654, 3.5444895123475089, 8.5690524057579651, 15.238357304868849, 21.789530155067276]
Normalized Reward history [1.276005283637702, 3.9913746703933821, 9.239380142826775, 16.132127620960595, 22.906743050181959]
