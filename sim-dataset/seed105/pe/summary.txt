Physical State
[[ 1.05  0.6 ]
 [ 1.1   0.6 ]
 [ 1.15  0.6 ]
 [ 1.2   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]
 [ 0.85  0.6 ]
 [ 0.9   0.6 ]
 [ 0.95  0.6 ]
 [ 1.    0.6 ]
 [ 1.05  0.6 ]
 [ 1.1   0.6 ]
 [ 1.15  0.6 ]
 [ 1.2   0.6 ]]
Measurements
[ 0.5656135   0.86230251  1.17554985  1.44675119  1.64829333  1.47371855
  1.25815961  1.05020584  0.87953226  1.06577201  1.11393928  1.03461525
  0.82295307  0.93048424  1.10624773  1.33151497  1.58284274  1.81570856
  2.00863052  2.12705933  2.15437066]
===============================================Measurements Collected
[array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.47371855,  1.25815961,  1.05020584,  0.87953226]), array([ 1.06577201,  1.11393928,  1.03461525,  0.82295307]), array([ 0.93048424,  1.10624773,  1.33151497,  1.58284274]), array([ 1.81570856,  2.00863052,  2.12705933,  2.15437066])]
Base measurements collected
[array([ 0.86230251,  1.17554985,  1.44675119,  1.64829333]), array([ 1.47371855,  1.25815961,  1.05020584,  0.87953226]), array([ 1.06577201,  1.11393928,  1.03461525,  0.82295307]), array([ 0.93048424,  1.10624773,  1.33151497,  1.58284274]), array([ 1.81570856,  2.00863052,  2.12705933,  2.15437066])]
Total accumulated reward = 26.8886514896
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [5.1328968685781478, 9.7945131346540037, 13.831792742172141, 18.782882417941845, 26.888651489622283]
Normalized Reward history [5.3563394476010844, 10.241398292699877, 14.502120479240951, 19.676652734033592, 28.005864384736967]
