Physical State
[[ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Measurements
[-0.09468722 -0.4296443  -0.67996952 -0.81780631 -0.83296254 -0.81780631
 -0.67996952 -0.4296443  -0.09468722  0.28166567  0.65464975  0.98969343
  1.26177424  1.44957866  1.55544615  1.60079867  1.58889585  1.60079867
  1.55544615  1.44957866  1.26177424]
===============================================Measurements Collected
[array([-0.4296443 , -0.67996952, -0.81780631, -0.83296254]), array([-0.81780631, -0.67996952, -0.4296443 , -0.09468722]), array([ 0.28166567,  0.65464975,  0.98969343,  1.26177424]), array([ 1.44957866,  1.55544615,  1.60079867,  1.58889585]), array([ 1.60079867,  1.55544615,  1.44957866,  1.26177424])]
Base measurements collected
[array([-0.4296443 , -0.67996952, -0.81780631, -0.83296254]), array([-0.81780631, -0.67996952, -0.4296443 , -0.09468722]), array([ 0.28166567,  0.65464975,  0.98969343,  1.26177424]), array([ 1.44957866,  1.55544615,  1.60079867,  1.58889585]), array([ 1.60079867,  1.55544615,  1.44957866,  1.26177424])]
Total accumulated reward = 10.4676101293
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-2.760382673319806, -4.7824900201165299, -1.5947069300802323, 4.6000124072602286, 10.467610129348088]
Normalized Reward history [-3.9311135779737727, -7.1239518294244624, -5.106899644042131, -0.082911211355637171, 4.6139556060782558]
