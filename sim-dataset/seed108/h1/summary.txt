Physical State
[[ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Measurements
[-0.09468722  0.28166567  0.65464975  0.98969343  1.26177424  1.44957866
  1.55544615  1.60079867  1.58889585  1.60079867  1.55544615  1.44957866
  1.26177424  1.44957866  1.55544615  1.60079867  1.58889585  1.60079867
  1.55544615  1.44957866  1.26177424]
===============================================Measurements Collected
[array([ 0.28166567,  0.65464975,  0.98969343,  1.26177424]), array([ 1.44957866,  1.55544615,  1.60079867,  1.58889585]), array([ 1.60079867,  1.55544615,  1.44957866,  1.26177424]), array([ 1.44957866,  1.55544615,  1.60079867,  1.58889585]), array([ 1.60079867,  1.55544615,  1.44957866,  1.26177424])]
Base measurements collected
[array([ 0.28166567,  0.65464975,  0.98969343,  1.26177424]), array([ 1.44957866,  1.55544615,  1.60079867,  1.58889585]), array([ 1.60079867,  1.55544615,  1.44957866,  1.26177424]), array([ 1.44957866,  1.55544615,  1.60079867,  1.58889585]), array([ 1.60079867,  1.55544615,  1.44957866,  1.26177424])]
Total accumulated reward = 27.3124172089
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [3.1877830900362976, 9.3825024273767585, 15.250100149464618, 21.444819486805081, 27.312417208892938]
Normalized Reward history [2.0170521853823313, 7.0410406180688252, 11.737907435502718, 16.761895868189214, 21.458762685623107]
