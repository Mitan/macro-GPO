Physical State
[[ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.05  1.4 ]
 [ 1.1   1.4 ]
 [ 1.15  1.4 ]
 [ 1.2   1.4 ]
 [ 1.15  1.4 ]
 [ 1.1   1.4 ]
 [ 1.05  1.4 ]
 [ 1.    1.4 ]
 [ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Measurements
[-0.09468722  0.28166567  0.65464975  0.98969343  1.26177424  1.44957866
  1.55544615  1.60079867  1.58889585  1.54246635  1.48389636  1.40906969
  1.32600284  1.40906969  1.48389636  1.54246635  1.58889585  1.61553659
  1.62692956  1.62757297  1.60544111]
===============================================Measurements Collected
[array([ 0.28166567,  0.65464975,  0.98969343,  1.26177424]), array([ 1.44957866,  1.55544615,  1.60079867,  1.58889585]), array([ 1.54246635,  1.48389636,  1.40906969,  1.32600284]), array([ 1.40906969,  1.48389636,  1.54246635,  1.58889585]), array([ 1.61553659,  1.62692956,  1.62757297,  1.60544111])]
Base measurements collected
[array([ 0.28166567,  0.65464975,  0.98969343,  1.26177424]), array([ 1.44957866,  1.55544615,  1.60079867,  1.58889585]), array([ 1.54246635,  1.48389636,  1.40906969,  1.32600284]), array([ 1.40906969,  1.48389636,  1.54246635,  1.58889585]), array([ 1.61553659,  1.62692956,  1.62757297,  1.60544111])]
Total accumulated reward = 27.6437461544
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [3.1877830900362976, 9.3825024273767585, 15.143937666628798, 21.16826592209075, 27.643746154398876]
Normalized Reward history [2.0170521853823313, 7.0410406180688252, 11.631744952666899, 16.485342303474884, 21.790091631129041]
