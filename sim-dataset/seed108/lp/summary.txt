Physical State
[[ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Measurements
[-0.09468722  0.0416239   0.13364888  0.13768604  0.05408978 -0.09090593
 -0.2662375  -0.42930659 -0.54629941 -0.42930659 -0.2662375  -0.09090593
  0.05408978  0.13768604  0.13364888  0.0416239  -0.09468722  0.0416239
  0.13364888  0.13768604  0.05408978]
===============================================Measurements Collected
[array([ 0.0416239 ,  0.13364888,  0.13768604,  0.05408978]), array([-0.09090593, -0.2662375 , -0.42930659, -0.54629941]), array([-0.42930659, -0.2662375 , -0.09090593,  0.05408978]), array([ 0.13768604,  0.13364888,  0.0416239 , -0.09468722]), array([ 0.0416239 ,  0.13364888,  0.13768604,  0.05408978])]
Base measurements collected
[array([ 0.0416239 ,  0.13364888,  0.13768604,  0.05408978]), array([-0.09090593, -0.2662375 , -0.42930659, -0.54629941]), array([-0.42930659, -0.2662375 , -0.09090593,  0.05408978]), array([ 0.13768604,  0.13364888,  0.0416239 , -0.09468722]), array([ 0.0416239 ,  0.13364888,  0.13768604,  0.05408978])]
Total accumulated reward = -1.11274086729
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.36704860235768394, -0.96570082947369573, -1.6980610766839581, -1.4797894696481777, -1.1127408672904937]
Normalized Reward history [-0.80368230229628246, -3.3071626387816284, -5.210253790645857, -6.1627130882640433, -6.9663953905603258]
