Physical State
[[ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.25  1.  ]
 [ 1.3   1.  ]
 [ 1.35  1.  ]
 [ 1.4   1.  ]
 [ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]
 [ 1.55  1.  ]
 [ 1.5   1.  ]
 [ 1.45  1.  ]
 [ 1.4   1.  ]
 [ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]]
Measurements
[-0.09468722 -0.25561579 -0.39241719 -0.47787199 -0.45797406 -0.33405294
 -0.14557783  0.09380896  0.31028101  0.44004074  0.45997429  0.35840649
  0.16343897  0.35840649  0.45997429  0.44004074  0.31028101  0.44004074
  0.45997429  0.35840649  0.16343897]
===============================================Measurements Collected
[array([-0.25561579, -0.39241719, -0.47787199, -0.45797406]), array([-0.33405294, -0.14557783,  0.09380896,  0.31028101]), array([ 0.44004074,  0.45997429,  0.35840649,  0.16343897]), array([ 0.35840649,  0.45997429,  0.44004074,  0.31028101]), array([ 0.44004074,  0.45997429,  0.35840649,  0.16343897])]
Base measurements collected
[array([-0.25561579, -0.39241719, -0.47787199, -0.45797406]), array([-0.33405294, -0.14557783,  0.09380896,  0.31028101]), array([ 0.44004074,  0.45997429,  0.35840649,  0.16343897]), array([ 0.35840649,  0.45997429,  0.44004074,  0.31028101]), array([ 0.44004074,  0.45997429,  0.35840649,  0.16343897])]
Total accumulated reward = 2.7530036944
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-1.583879029329913, -1.6594198308495676, -0.23755933661344431, 1.3311432001621522, 2.7530036943982754]
Normalized Reward history [-2.7546099339838794, -4.0008816401575, -3.749752050575343, -3.3517804184537132, -3.1006508288715562]
