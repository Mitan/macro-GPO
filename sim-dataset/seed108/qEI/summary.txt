Physical State
[[ 1.4   0.55]
 [ 1.4   0.5 ]
 [ 1.4   0.45]
 [ 1.4   0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.25  0.8 ]
 [ 1.3   0.8 ]
 [ 1.35  0.8 ]
 [ 1.4   0.8 ]
 [ 1.4   0.75]
 [ 1.4   0.7 ]
 [ 1.4   0.65]
 [ 1.4   0.6 ]
 [ 1.4   0.55]
 [ 1.4   0.5 ]
 [ 1.4   0.45]
 [ 1.4   0.4 ]]
Measurements
[-0.09468722 -0.4296443  -0.67996952 -0.81780631 -0.83296254 -0.85737433
 -0.84730021 -0.79820536 -0.65015029 -0.396305   -0.05103956  0.33591145
  0.73778552  1.04195777  1.39053144  1.74276289  2.05418498  2.25357589
  2.29749486  2.15646762  1.83053248]
===============================================Measurements Collected
[array([-0.4296443 , -0.67996952, -0.81780631, -0.83296254]), array([-0.85737433, -0.84730021, -0.79820536, -0.65015029]), array([-0.396305  , -0.05103956,  0.33591145,  0.73778552]), array([ 1.04195777,  1.39053144,  1.74276289,  2.05418498]), array([ 2.25357589,  2.29749486,  2.15646762,  1.83053248])]
Base measurements collected
[array([-0.4296443 , -0.67996952, -0.81780631, -0.83296254]), array([-0.85737433, -0.84730021, -0.79820536, -0.65015029]), array([-0.396305  , -0.05103956,  0.33591145,  0.73778552]), array([ 1.04195777,  1.39053144,  1.74276289,  2.05418498]), array([ 2.25357589,  2.29749486,  2.15646762,  1.83053248])]
Total accumulated reward = 9.48044748249
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [-2.760382673319806, -5.9134128558975689, -5.2870604538490626, 0.94237662670370081, 9.4804474824862979]
Normalized Reward history [-3.9311135779737727, -8.2548746652055023, -8.7992531678109636, -3.7405469919121668, 3.6267929592164645]
