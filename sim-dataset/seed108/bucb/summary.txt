Physical State
[[ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.8   1.25]
 [ 0.8   1.3 ]
 [ 0.8   1.35]
 [ 0.8   1.4 ]
 [ 0.85  1.4 ]
 [ 0.9   1.4 ]
 [ 0.95  1.4 ]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Measurements
[-0.09468722  0.28166567  0.65464975  0.98969343  1.26177424  1.38362007
  1.44598043  1.44125091  1.34896431  1.47787326  1.54904664  1.58275227
  1.60544111  1.62757297  1.62692956  1.61553659  1.58889585  1.60079867
  1.55544615  1.44957866  1.26177424]
===============================================Measurements Collected
[array([ 0.28166567,  0.65464975,  0.98969343,  1.26177424]), array([ 1.38362007,  1.44598043,  1.44125091,  1.34896431]), array([ 1.47787326,  1.54904664,  1.58275227,  1.60544111]), array([ 1.62757297,  1.62692956,  1.61553659,  1.58889585]), array([ 1.60079867,  1.55544615,  1.44957866,  1.26177424])]
Base measurements collected
[array([ 0.28166567,  0.65464975,  0.98969343,  1.26177424]), array([ 1.38362007,  1.44598043,  1.44125091,  1.34896431]), array([ 1.47787326,  1.54904664,  1.58275227,  1.60544111]), array([ 1.62757297,  1.62692956,  1.61553659,  1.58889585]), array([ 1.60079867,  1.55544615,  1.44957866,  1.26177424])]
Total accumulated reward = 27.3492447764
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [3.1877830900362976, 8.8075988034381751, 15.022712079209033, 21.481647054327837, 27.349244776415695]
Normalized Reward history [2.0170521853823313, 6.4661369941302418, 11.510519365247131, 16.79872343571197, 21.495590253145863]
