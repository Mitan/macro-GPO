Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 0.47184674  0.65171033  0.77204081  0.82373349  0.78806029  0.72558802
  0.5361189   0.2140179  -0.21063645 -0.22263285 -0.24967179 -0.26149361
 -0.24967116 -0.04658425  0.14241858  0.32582447  0.47184674  0.65171033
  0.77204081  0.82373349  0.78806029]
===============================================Measurements Collected
[array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.72558802,  0.5361189 ,  0.2140179 , -0.21063645]), array([-0.22263285, -0.24967179, -0.26149361, -0.24967116]), array([-0.04658425,  0.14241858,  0.32582447,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029])]
Base measurements collected
[array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.72558802,  0.5361189 ,  0.2140179 , -0.21063645]), array([-0.22263285, -0.24967179, -0.26149361, -0.24967116]), array([-0.04658425,  0.14241858,  0.32582447,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029])]
Total accumulated reward = 7.24621430129
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [3.0355449086119286, 4.3006332737334363, 3.3171638565576584, 4.2106693926756087, 7.2462143012875373]
Normalized Reward history [2.9020132493779678, 4.0335699552655147, 2.9165688788557764, 3.6765427557397654, 6.5785560051177328]
