Physical State
[[ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]]
Measurements
[ 0.47184674  0.566184    0.58029973  0.53660547  0.40322607  0.21299826
  0.00190609 -0.20315513 -0.35811428 -0.20315513  0.00190609  0.21299826
  0.40322607  0.21299826  0.00190609 -0.20315513 -0.35811428 -0.20315513
  0.00190609  0.21299826  0.40322607]
===============================================Measurements Collected
[array([ 0.566184  ,  0.58029973,  0.53660547,  0.40322607]), array([ 0.21299826,  0.00190609, -0.20315513, -0.35811428]), array([-0.20315513,  0.00190609,  0.21299826,  0.40322607]), array([ 0.21299826,  0.00190609, -0.20315513, -0.35811428]), array([-0.20315513,  0.00190609,  0.21299826,  0.40322607])]
Base measurements collected
[array([ 0.566184  ,  0.58029973,  0.53660547,  0.40322607]), array([ 0.21299826,  0.00190609, -0.20315513, -0.35811428]), array([-0.20315513,  0.00190609,  0.21299826,  0.40322607]), array([ 0.21299826,  0.00190609, -0.20315513, -0.35811428]), array([-0.20315513,  0.00190609,  0.21299826,  0.40322607])]
Total accumulated reward = 2.22353571505
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [2.0863152612652809, 1.7399502036593293, 2.154925488159352, 1.8085604305534004, 2.2235357150534232]
Normalized Reward history [1.9527836020313201, 1.4728868851914076, 1.7543305104574696, 1.2744337936175572, 1.5558774188836191]
