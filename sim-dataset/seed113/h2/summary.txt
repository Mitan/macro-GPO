Physical State
[[ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]]
Measurements
[ 0.47184674  0.32582447  0.14241858 -0.04658425 -0.24967116 -0.04658425
  0.14241858  0.32582447  0.47184674  0.566184    0.58029973  0.53660547
  0.40322607  0.53660547  0.58029973  0.566184    0.47184674  0.566184
  0.58029973  0.53660547  0.40322607]
===============================================Measurements Collected
[array([ 0.32582447,  0.14241858, -0.04658425, -0.24967116]), array([-0.04658425,  0.14241858,  0.32582447,  0.47184674]), array([ 0.566184  ,  0.58029973,  0.53660547,  0.40322607]), array([ 0.53660547,  0.58029973,  0.566184  ,  0.47184674]), array([ 0.566184  ,  0.58029973,  0.53660547,  0.40322607])]
Base measurements collected
[array([ 0.32582447,  0.14241858, -0.04658425, -0.24967116]), array([-0.04658425,  0.14241858,  0.32582447,  0.47184674]), array([ 0.566184  ,  0.58029973,  0.53660547,  0.40322607]), array([ 0.53660547,  0.58029973,  0.566184  ,  0.47184674]), array([ 0.566184  ,  0.58029973,  0.53660547,  0.40322607])]
Total accumulated reward = 7.39305962625
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.17198763099304543, 1.0654931671109953, 3.1518084283762762, 5.3067443649850272, 7.3930596262503077]
Normalized Reward history [0.038455971759084606, 0.79842984864307365, 2.7512134506743937, 4.7726177280491839, 6.725401330080504]
