Physical State
[[ 1.05  0.6 ]
 [ 1.1   0.6 ]
 [ 1.15  0.6 ]
 [ 1.2   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.15  0.8 ]
 [ 1.1   0.8 ]
 [ 1.05  0.8 ]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.05  0.6 ]
 [ 1.1   0.6 ]
 [ 1.15  0.6 ]
 [ 1.2   0.6 ]]
Measurements
[ 0.47184674  0.32582447  0.14241858 -0.04658425 -0.24967116 -0.26149361
 -0.24967179 -0.22263285 -0.21063645  0.2140179   0.5361189   0.72558802
  0.78806029  0.68698069  0.52439651  0.33144578  0.13586719  0.16517658
  0.10107766 -0.07309572 -0.32423699]
===============================================Measurements Collected
[array([ 0.32582447,  0.14241858, -0.04658425, -0.24967116]), array([-0.26149361, -0.24967179, -0.22263285, -0.21063645]), array([ 0.2140179 ,  0.5361189 ,  0.72558802,  0.78806029]), array([ 0.68698069,  0.52439651,  0.33144578,  0.13586719]), array([ 0.16517658,  0.10107766, -0.07309572, -0.32423699])]
Base measurements collected
[array([ 0.32582447,  0.14241858, -0.04658425, -0.24967116]), array([-0.26149361, -0.24967179, -0.22263285, -0.21063645]), array([ 0.2140179 ,  0.5361189 ,  0.72558802,  0.78806029]), array([ 0.68698069,  0.52439651,  0.33144578,  0.13586719]), array([ 0.16517658,  0.10107766, -0.07309572, -0.32423699])]
Total accumulated reward = 3.03894972095
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.17198763099304543, -0.77244707385405609, 1.4913380290174498, 3.1700281963636758, 3.0389497209505407]
Normalized Reward history [0.038455971759084606, -1.0395103923219777, 1.0907430513155674, 2.6359015594278326, 2.3712914247807366]
