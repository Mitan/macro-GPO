Physical State
[[ 1.15  0.8 ]
 [ 1.1   0.8 ]
 [ 1.05  0.8 ]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.15  0.8 ]
 [ 1.1   0.8 ]
 [ 1.05  0.8 ]
 [ 1.    0.8 ]]
Measurements
[ 0.47184674  0.26526985  0.05091935 -0.14707016 -0.3039348  -0.14707016
  0.05091935  0.26526985  0.47184674  0.65171033  0.77204081  0.82373349
  0.78806029  0.72558802  0.5361189   0.2140179  -0.21063645  0.2140179
  0.5361189   0.72558802  0.78806029]
===============================================Measurements Collected
[array([ 0.26526985,  0.05091935, -0.14707016, -0.3039348 ]), array([-0.14707016,  0.05091935,  0.26526985,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.72558802,  0.5361189 ,  0.2140179 , -0.21063645]), array([ 0.2140179 ,  0.5361189 ,  0.72558802,  0.78806029])]
Base measurements collected
[array([ 0.26526985,  0.05091935, -0.14707016, -0.3039348 ]), array([-0.14707016,  0.05091935,  0.26526985,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.72558802,  0.5361189 ,  0.2140179 , -0.21063645]), array([ 0.2140179 ,  0.5361189 ,  0.72558802,  0.78806029])]
Total accumulated reward = 7.07056841973
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-0.13481574915168559, 0.50615004312130152, 3.5416949517332301, 4.8067833168547374, 7.0705684197262428]
Normalized Reward history [-0.26834740838564641, 0.23908672465337988, 3.1410999740313477, 4.2726566799188941, 6.4029101235564392]
