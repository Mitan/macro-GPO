Physical State
[[ 0.85  0.8 ]
 [ 0.9   0.8 ]
 [ 0.95  0.8 ]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.85  0.8 ]
 [ 0.9   0.8 ]
 [ 0.95  0.8 ]
 [ 1.    0.8 ]]
Measurements
[ 0.47184674  0.26526985  0.05091935 -0.14707016 -0.3039348  -0.14707016
  0.05091935  0.26526985  0.47184674  0.65171033  0.77204081  0.82373349
  0.78806029  0.72441193  0.56455962  0.36166646  0.14948545  0.36166646
  0.56455962  0.72441193  0.78806029]
===============================================Measurements Collected
[array([ 0.26526985,  0.05091935, -0.14707016, -0.3039348 ]), array([-0.14707016,  0.05091935,  0.26526985,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.72441193,  0.56455962,  0.36166646,  0.14948545]), array([ 0.36166646,  0.56455962,  0.72441193,  0.78806029])]
Base measurements collected
[array([ 0.26526985,  0.05091935, -0.14707016, -0.3039348 ]), array([-0.14707016,  0.05091935,  0.26526985,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.72441193,  0.56455962,  0.36166646,  0.14948545]), array([ 0.36166646,  0.56455962,  0.72441193,  0.78806029])]
Total accumulated reward = 7.78051672621
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-0.13481574915168559, 0.50615004312130152, 3.5416949517332301, 5.3418184228121275, 7.7805167262092443]
Normalized Reward history [-0.26834740838564641, 0.23908672465337988, 3.1410999740313477, 4.8076917858762842, 7.1128584300394397]
