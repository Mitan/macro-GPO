Physical State
[[ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.15  0.8 ]
 [ 1.1   0.8 ]
 [ 1.05  0.8 ]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]]
Measurements
[ 0.47184674  0.32582447  0.14241858 -0.04658425 -0.24967116 -0.26149361
 -0.24967179 -0.22263285 -0.21063645  0.2140179   0.5361189   0.72558802
  0.78806029  0.72441193  0.56455962  0.36166646  0.14948545  0.0097765
 -0.09836344 -0.17838774 -0.22573349]
===============================================Measurements Collected
[array([ 0.32582447,  0.14241858, -0.04658425, -0.24967116]), array([-0.26149361, -0.24967179, -0.22263285, -0.21063645]), array([ 0.2140179 ,  0.5361189 ,  0.72558802,  0.78806029]), array([ 0.72441193,  0.56455962,  0.36166646,  0.14948545]), array([ 0.0097765 , -0.09836344, -0.17838774, -0.22573349])]
Base measurements collected
[array([ 0.32582447,  0.14241858, -0.04658425, -0.24967116]), array([-0.26149361, -0.24967179, -0.22263285, -0.21063645]), array([ 0.2140179 ,  0.5361189 ,  0.72558802,  0.78806029]), array([ 0.72441193,  0.56455962,  0.36166646,  0.14948545]), array([ 0.0097765 , -0.09836344, -0.17838774, -0.22573349])]
Total accumulated reward = 2.79875334796
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [0.17198763099304543, -0.77244707385405609, 1.4913380290174498, 3.2914615000963474, 2.7987533479572089]
Normalized Reward history [0.038455971759084606, -1.0395103923219777, 1.0907430513155674, 2.7573348631605041, 2.1310950517874048]
