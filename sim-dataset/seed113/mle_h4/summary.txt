Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 0.47184674  0.65171033  0.77204081  0.82373349  0.78806029  0.82373349
  0.77204081  0.65171033  0.47184674  0.65171033  0.77204081  0.82373349
  0.78806029  0.82373349  0.77204081  0.65171033  0.47184674  0.65171033
  0.77204081  0.82373349  0.78806029]
===============================================Measurements Collected
[array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.82373349,  0.77204081,  0.65171033,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.82373349,  0.77204081,  0.65171033,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029])]
Base measurements collected
[array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.82373349,  0.77204081,  0.65171033,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029]), array([ 0.82373349,  0.77204081,  0.65171033,  0.47184674]), array([ 0.65171033,  0.77204081,  0.82373349,  0.78806029])]
Total accumulated reward = 14.5452974532
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [3.0355449086119286, 5.7548762722700877, 8.7904211808820172, 11.509752544540177, 14.545297453152106]
Normalized Reward history [2.9020132493779678, 5.487812953802166, 8.3898262031801334, 10.975625907604332, 13.877639156982299]
