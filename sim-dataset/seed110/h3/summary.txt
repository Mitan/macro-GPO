Physical State
[[ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.65  1.  ]
 [ 0.7   1.  ]
 [ 0.75  1.  ]
 [ 0.8   1.  ]]
Measurements
[-0.24804094 -0.45451742 -0.63743021 -0.80919221 -0.96246588 -0.80919221
 -0.63743021 -0.45451742 -0.24804094 -0.01953251  0.22526531  0.47649905
  0.71204733  0.88808463  0.99433671  1.00409213  0.93507562  1.00409213
  0.99433671  0.88808463  0.71204733]
===============================================Measurements Collected
[array([-0.45451742, -0.63743021, -0.80919221, -0.96246588]), array([-0.80919221, -0.63743021, -0.45451742, -0.24804094]), array([-0.01953251,  0.22526531,  0.47649905,  0.71204733]), array([ 0.88808463,  0.99433671,  1.00409213,  0.93507562]), array([ 1.00409213,  0.99433671,  0.88808463,  0.71204733])]
Base measurements collected
[array([-0.45451742, -0.63743021, -0.80919221, -0.96246588]), array([-0.80919221, -0.63743021, -0.45451742, -0.24804094]), array([-0.01953251,  0.22526531,  0.47649905,  0.71204733]), array([ 0.88808463,  0.99433671,  1.00409213,  0.93507562]), array([ 1.00409213,  0.99433671,  0.88808463,  0.71204733])]
Total accumulated reward = 3.80164258628
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-2.8636057134655557, -5.0127864901929549, -3.6185073170483695, 0.20308178065731264, 3.8016425862797023]
Normalized Reward history [-2.2393398578747439, -3.7642547790113312, -1.745709750275934, 2.7001452030205599, 6.9229718642337623]
