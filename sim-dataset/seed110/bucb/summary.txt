Physical State
[[ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   1.05]
 [ 1.2   1.1 ]
 [ 1.2   1.15]
 [ 1.2   1.2 ]
 [ 1.15  1.2 ]
 [ 1.1   1.2 ]
 [ 1.05  1.2 ]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 0.95  1.4 ]
 [ 0.9   1.4 ]
 [ 0.85  1.4 ]
 [ 0.8   1.4 ]]
Measurements
[-0.24804094 -0.45451742 -0.63743021 -0.80919221 -0.96246588 -0.54986095
 -0.1616977   0.15163587  0.38611962  0.73191301  1.08485632  1.38015384
  1.58545926  1.79005643  1.90214411  1.95084538  1.97459161  2.04993287
  1.97795422  1.7735527   1.47359208]
===============================================Measurements Collected
[array([-0.45451742, -0.63743021, -0.80919221, -0.96246588]), array([-0.54986095, -0.1616977 ,  0.15163587,  0.38611962]), array([ 0.73191301,  1.08485632,  1.38015384,  1.58545926]), array([ 1.79005643,  1.90214411,  1.95084538,  1.97459161]), array([ 2.04993287,  1.97795422,  1.7735527 ,  1.47359208])]
Base measurements collected
[array([-0.45451742, -0.63743021, -0.80919221, -0.96246588]), array([-0.54986095, -0.1616977 ,  0.15163587,  0.38611962]), array([ 0.73191301,  1.08485632,  1.38015384,  1.58545926]), array([ 1.79005643,  1.90214411,  1.95084538,  1.97459161]), array([ 2.04993287,  1.97795422,  1.7735527 ,  1.47359208])]
Total accumulated reward = 16.6376429354
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-2.8636057134655557, -3.0374088874456424, 1.74497353773189, 9.3626110606672874, 16.637642935405246]
Normalized Reward history [-2.2393398578747439, -1.7888771762640188, 3.6177711045043255, 11.859674483030535, 19.758972213359307]
