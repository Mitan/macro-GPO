Physical State
[[ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]]
Measurements
[-0.24804094 -0.45451742 -0.63743021 -0.80919221 -0.96246588 -0.80919221
 -0.63743021 -0.45451742 -0.24804094 -0.45451742 -0.63743021 -0.80919221
 -0.96246588 -0.80919221 -0.63743021 -0.45451742 -0.24804094 -0.45451742
 -0.63743021 -0.80919221 -0.96246588]
===============================================Measurements Collected
[array([-0.45451742, -0.63743021, -0.80919221, -0.96246588]), array([-0.80919221, -0.63743021, -0.45451742, -0.24804094]), array([-0.45451742, -0.63743021, -0.80919221, -0.96246588]), array([-0.80919221, -0.63743021, -0.45451742, -0.24804094]), array([-0.45451742, -0.63743021, -0.80919221, -0.96246588])]
Base measurements collected
[array([-0.45451742, -0.63743021, -0.80919221, -0.96246588]), array([-0.80919221, -0.63743021, -0.45451742, -0.24804094]), array([-0.45451742, -0.63743021, -0.80919221, -0.96246588]), array([-0.80919221, -0.63743021, -0.45451742, -0.24804094]), array([-0.45451742, -0.63743021, -0.80919221, -0.96246588])]
Total accumulated reward = -12.8891786939
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-2.8636057134655557, -5.0127864901929549, -7.8763922036585106, -10.02557298038591, -12.889178693851466]
Normalized Reward history [-2.2393398578747439, -3.7642547790113312, -6.0035946368860751, -7.5285095580226624, -9.7678494158974054]
