Physical State
[[ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]
 [ 1.    1.55]
 [ 1.    1.5 ]
 [ 1.    1.45]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Measurements
[-0.24804094  0.34948804  0.86458187  1.291052    1.58545926  1.79005643
  1.90214411  1.95084538  1.97459161  1.99258095  2.0125373   2.02757499
  2.01447878  2.02757499  2.0125373   1.99258095  1.97459161  1.99258095
  2.0125373   2.02757499  2.01447878]
===============================================Measurements Collected
[array([ 0.34948804,  0.86458187,  1.291052  ,  1.58545926]), array([ 1.79005643,  1.90214411,  1.95084538,  1.97459161]), array([ 1.99258095,  2.0125373 ,  2.02757499,  2.01447878]), array([ 2.02757499,  2.0125373 ,  1.99258095,  1.97459161]), array([ 1.99258095,  2.0125373 ,  2.02757499,  2.01447878])]
Base measurements collected
[array([ 0.34948804,  0.86458187,  1.291052  ,  1.58545926]), array([ 1.79005643,  1.90214411,  1.95084538,  1.97459161]), array([ 1.99258095,  2.0125373 ,  2.02757499,  2.01447878]), array([ 2.02757499,  2.0125373 ,  1.99258095,  1.97459161]), array([ 1.99258095,  2.0125373 ,  2.02757499,  2.01447878])]
Total accumulated reward = 35.8098475784
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [4.0905811695058798, 11.708218692441278, 19.755390712464425, 27.762675558351276, 35.809847578374423]
Normalized Reward history [4.7148470250966916, 12.956750403622902, 21.628188279236859, 30.259738980714523, 38.931176856328477]
