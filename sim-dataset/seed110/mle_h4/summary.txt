Physical State
[[ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Measurements
[-0.24804094 -0.87409305 -1.46327899 -1.94462126 -2.24285856 -1.94462126
 -1.46327899 -0.87409305 -0.24804094  0.34948804  0.86458187  1.291052
  1.58545926  1.79005643  1.90214411  1.95084538  1.97459161  1.99258095
  2.0125373   2.02757499  2.01447878]
===============================================Measurements Collected
[array([-0.87409305, -1.46327899, -1.94462126, -2.24285856]), array([-1.94462126, -1.46327899, -0.87409305, -0.24804094]), array([ 0.34948804,  0.86458187,  1.291052  ,  1.58545926]), array([ 1.79005643,  1.90214411,  1.95084538,  1.97459161]), array([ 1.99258095,  2.0125373 ,  2.02757499,  2.01447878])]
Base measurements collected
[array([-0.87409305, -1.46327899, -1.94462126, -2.24285856]), array([-1.94462126, -1.46327899, -0.87409305, -0.24804094]), array([ 0.34948804,  0.86458187,  1.291052  ,  1.58545926]), array([ 1.79005643,  1.90214411,  1.95084538,  1.97459161]), array([ 1.99258095,  2.0125373 ,  2.02757499,  2.01447878])]
Total accumulated reward = 8.70050459673
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-6.5248518646886469, -11.054886115738022, -6.9643049462321418, 0.65333257670325651, 8.7005045967264021]
Normalized Reward history [-5.900586009097835, -9.806354404556398, -5.0915073794597063, 3.1503959990665038, 11.821833874680461]
