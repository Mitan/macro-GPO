Physical State
[[ 0.65  0.6 ]
 [ 0.7   0.6 ]
 [ 0.75  0.6 ]
 [ 0.8   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   0.95]
 [ 0.8   0.9 ]
 [ 0.8   0.85]
 [ 0.8   0.8 ]
 [ 0.75  0.8 ]
 [ 0.7   0.8 ]
 [ 0.65  0.8 ]
 [ 0.6   0.8 ]
 [ 0.6   0.75]
 [ 0.6   0.7 ]
 [ 0.6   0.65]
 [ 0.6   0.6 ]
 [ 0.65  0.6 ]
 [ 0.7   0.6 ]
 [ 0.75  0.6 ]
 [ 0.8   0.6 ]]
Measurements
[-0.24804094 -0.01953251  0.22526531  0.47649905  0.71204733  0.46225847
  0.23529281  0.05761673 -0.05424298  0.37627012  0.65763915  0.77233264
  0.75768828  0.68541617  0.64361725  0.63631119  0.65306582  0.71047506
  0.69625078  0.55999814  0.29401628]
===============================================Measurements Collected
[array([-0.01953251,  0.22526531,  0.47649905,  0.71204733]), array([ 0.46225847,  0.23529281,  0.05761673, -0.05424298]), array([ 0.37627012,  0.65763915,  0.77233264,  0.75768828]), array([ 0.68541617,  0.64361725,  0.63631119,  0.65306582]), array([ 0.71047506,  0.69625078,  0.55999814,  0.29401628])]
Base measurements collected
[array([-0.01953251,  0.22526531,  0.47649905,  0.71204733]), array([ 0.46225847,  0.23529281,  0.05761673, -0.05424298]), array([ 0.37627012,  0.65763915,  0.77233264,  0.75768828]), array([ 0.68541617,  0.64361725,  0.63631119,  0.65306582]), array([ 0.71047506,  0.69625078,  0.55999814,  0.29401628])]
Total accumulated reward = 9.53828507961
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [1.3942791731445854, 2.0952041975846072, 4.6591343859464516, 7.2775448153169311, 9.5382850796132992]
Normalized Reward history [2.0185450287353972, 3.3437359087662308, 6.531931952718887, 9.7746082376801784, 12.659614357567358]
