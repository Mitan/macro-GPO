Physical State
[[ 0.6   0.55]
 [ 0.6   0.5 ]
 [ 0.6   0.45]
 [ 0.6   0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]
 [ 0.75  0.6 ]
 [ 0.7   0.6 ]
 [ 0.65  0.6 ]
 [ 0.6   0.6 ]
 [ 0.6   0.55]
 [ 0.6   0.5 ]
 [ 0.6   0.45]
 [ 0.6   0.4 ]]
Measurements
[-0.57287335 -0.65683592 -0.65240963 -0.56622431 -0.40290631 -0.41032104
 -0.40967616 -0.41506869 -0.43649219 -0.00981601  0.40161081  0.75602937
  1.03127295  1.10309434  1.07178408  0.92132497  0.63681038  0.74108376
  0.64859998  0.38923855 -0.01176109]
===============================================Measurements Collected
[array([-0.65683592, -0.65240963, -0.56622431, -0.40290631]), array([-0.41032104, -0.40967616, -0.41506869, -0.43649219]), array([-0.00981601,  0.40161081,  0.75602937,  1.03127295]), array([ 1.10309434,  1.07178408,  0.92132497,  0.63681038]), array([ 0.74108376,  0.64859998,  0.38923855, -0.01176109])]
Base measurements collected
[array([-0.65683592, -0.65240963, -0.56622431, -0.40290631]), array([-0.41032104, -0.40967616, -0.41506869, -0.43649219]), array([-0.00981601,  0.40161081,  0.75602937,  1.03127295]), array([ 1.10309434,  1.07178408,  0.92132497,  0.63681038]), array([ 0.74108376,  0.64859998,  0.38923855, -0.01176109])]
Total accumulated reward = 3.7293378705
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-2.2783761589154121, -3.9499342287214851, -1.7708371081682994, 1.9621766681647101, 3.7293378704997737]
Normalized Reward history [-0.29302120438802604, 0.020775680333287028, 4.1852277554138588, 9.9035964862742549, 13.656112643136705]
