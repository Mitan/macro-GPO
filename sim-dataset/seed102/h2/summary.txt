Physical State
[[ 1.15  0.4 ]
 [ 1.1   0.4 ]
 [ 1.05  0.4 ]
 [ 1.    0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.55]
 [ 1.    0.5 ]
 [ 1.    0.45]
 [ 1.    0.4 ]
 [ 1.05  0.4 ]
 [ 1.1   0.4 ]
 [ 1.15  0.4 ]
 [ 1.2   0.4 ]
 [ 1.15  0.4 ]
 [ 1.1   0.4 ]
 [ 1.05  0.4 ]
 [ 1.    0.4 ]]
Measurements
[-0.57287335 -0.65683592 -0.65240963 -0.56622431 -0.40290631 -0.21852886
 -0.02371925  0.17270179  0.36123529  0.53006036  0.6871577   0.78258883
  0.82888194  0.72249661  0.58687767  0.40959627  0.1942667   0.40959627
  0.58687767  0.72249661  0.82888194]
===============================================Measurements Collected
[array([-0.65683592, -0.65240963, -0.56622431, -0.40290631]), array([-0.21852886, -0.02371925,  0.17270179,  0.36123529]), array([ 0.53006036,  0.6871577 ,  0.78258883,  0.82888194]), array([ 0.72249661,  0.58687767,  0.40959627,  0.1942667 ]), array([ 0.40959627,  0.58687767,  0.72249661,  0.82888194])]
Base measurements collected
[array([-0.65683592, -0.65240963, -0.56622431, -0.40290631]), array([-0.21852886, -0.02371925,  0.17270179,  0.36123529]), array([ 0.53006036,  0.6871577 ,  0.78258883,  0.82888194]), array([ 0.72249661,  0.58687767,  0.40959627,  0.1942667 ]), array([ 0.40959627,  0.58687767,  0.72249661,  0.82888194])]
Total accumulated reward = 5.3030913763
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-2.2783761589154121, -1.9866871921792835, 0.84200163919770565, 2.7552388894269457, 5.3030913763020058]
Normalized Reward history [-0.29302120438802604, 1.9840227168754887, 6.7980665027798644, 10.696658707536491, 15.229866148938937]
