Physical State
[[ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.25  1.  ]
 [ 1.3   1.  ]
 [ 1.35  1.  ]
 [ 1.4   1.  ]
 [ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]
 [ 1.55  1.  ]
 [ 1.5   1.  ]
 [ 1.45  1.  ]
 [ 1.4   1.  ]
 [ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]]
Measurements
[-0.57287335 -0.29449398 -0.01239151  0.27735296  0.57384111  0.86185451
  1.10263962  1.30426169  1.43519994  1.48565321  1.45097061  1.31392635
  1.08596735  1.31392635  1.45097061  1.48565321  1.43519994  1.48565321
  1.45097061  1.31392635  1.08596735]
===============================================Measurements Collected
[array([-0.29449398, -0.01239151,  0.27735296,  0.57384111]), array([ 0.86185451,  1.10263962,  1.30426169,  1.43519994]), array([ 1.48565321,  1.45097061,  1.31392635,  1.08596735]), array([ 1.31392635,  1.45097061,  1.48565321,  1.43519994]), array([ 1.48565321,  1.45097061,  1.31392635,  1.08596735])]
Base measurements collected
[array([-0.29449398, -0.01239151,  0.27735296,  0.57384111]), array([ 0.86185451,  1.10263962,  1.30426169,  1.43519994]), array([ 1.48565321,  1.45097061,  1.31392635,  1.08596735]), array([ 1.31392635,  1.45097061,  1.48565321,  1.43519994]), array([ 1.48565321,  1.45097061,  1.31392635,  1.08596735])]
Total accumulated reward = 21.6070494685
Nodes Expanded per stage
[118394, 104794, 22362, 6001, 5]
Total nodes expanded = 251556
Reward history [0.5443085846105078, 5.2482643402744289, 10.584781852906332, 16.270531955887385, 21.607049468519286]
Normalized Reward history [2.5296635391378937, 9.2189742493292002, 16.540846716488488, 24.21195177399693, 31.533824241156218]
