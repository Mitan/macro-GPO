Physical State
[[ 0.6   0.95]
 [ 0.6   0.9 ]
 [ 0.6   0.85]
 [ 0.6   0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.85]
 [ 0.8   0.9 ]
 [ 0.8   0.95]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.6   0.95]
 [ 0.6   0.9 ]
 [ 0.6   0.85]
 [ 0.6   0.8 ]]
Measurements
[-0.57287335 -0.65683592 -0.65240963 -0.56622431 -0.40290631 -0.41032104
 -0.40967616 -0.41506869 -0.43649219 -0.8536546  -1.224389   -1.53423114
 -1.77496243 -2.06332538 -2.28201947 -2.39942104 -2.42722809 -2.14807504
 -1.79560946 -1.39223623 -0.94573724]
===============================================Measurements Collected
[array([-0.65683592, -0.65240963, -0.56622431, -0.40290631]), array([-0.41032104, -0.40967616, -0.41506869, -0.43649219]), array([-0.8536546 , -1.224389  , -1.53423114, -1.77496243]), array([-2.06332538, -2.28201947, -2.39942104, -2.42722809]), array([-2.14807504, -1.79560946, -1.39223623, -0.94573724])]
Base measurements collected
[array([-0.65683592, -0.65240963, -0.56622431, -0.40290631]), array([-0.41032104, -0.40967616, -0.41506869, -0.43649219]), array([-0.8536546 , -1.224389  , -1.53423114, -1.77496243]), array([-2.06332538, -2.28201947, -2.39942104, -2.42722809]), array([-2.14807504, -1.79560946, -1.39223623, -0.94573724])]
Total accumulated reward = -24.7908233654
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [-2.2783761589154121, -3.9499342287214851, -9.3371713980704065, -18.509165385894825, -24.790823365382128]
Normalized Reward history [-0.29302120438802604, 0.020775680333287028, -3.3811065344882483, -10.567745567785281, -14.864048592745196]
