Physical State
[[ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.35]
 [ 1.    1.3 ]
 [ 1.    1.25]
 [ 1.    1.2 ]]
Measurements
[-0.57287335 -0.42500729 -0.22749731 -0.04074566  0.10655097  0.19142602
  0.19713283  0.11635202 -0.02390211  0.11635202  0.19713283  0.19142602
  0.10655097  0.19142602  0.19713283  0.11635202 -0.02390211  0.11635202
  0.19713283  0.19142602  0.10655097]
===============================================Measurements Collected
[array([-0.42500729, -0.22749731, -0.04074566,  0.10655097]), array([ 0.19142602,  0.19713283,  0.11635202, -0.02390211]), array([ 0.11635202,  0.19713283,  0.19142602,  0.10655097]), array([ 0.19142602,  0.19713283,  0.11635202, -0.02390211]), array([ 0.11635202,  0.19713283,  0.19142602,  0.10655097])]
Base measurements collected
[array([-0.42500729, -0.22749731, -0.04074566,  0.10655097]), array([ 0.19142602,  0.19713283,  0.11635202, -0.02390211]), array([ 0.11635202,  0.19713283,  0.19142602,  0.10655097]), array([ 0.19142602,  0.19713283,  0.11635202, -0.02390211]), array([ 0.11635202,  0.19713283,  0.19142602,  0.10655097])]
Total accumulated reward = 1.59824190375
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-0.58669929867919424, -0.10569053603356054, 0.50577130253743463, 0.98678006518306827, 1.5982419037540634]
Normalized Reward history [1.398655655848192, 3.8650193730212119, 6.4618361661195927, 8.9281998832926135, 11.525016676390994]
