Physical State
[[ 0.6   1.05]
 [ 0.6   1.1 ]
 [ 0.6   1.15]
 [ 0.6   1.2 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.85  1.  ]
 [ 0.9   1.  ]
 [ 0.95  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.75  1.  ]
 [ 0.7   1.  ]
 [ 0.65  1.  ]
 [ 0.6   1.  ]
 [ 0.6   1.05]
 [ 0.6   1.1 ]
 [ 0.6   1.15]
 [ 0.6   1.2 ]]
Measurements
[-0.57287335 -0.85731539 -1.15793805 -1.47062238 -1.77496243 -1.47062238
 -1.15793805 -0.85731539 -0.57287335 -0.85731539 -1.15793805 -1.47062238
 -1.77496243 -2.06332538 -2.28201947 -2.39942104 -2.42722809 -2.60679256
 -2.6584211  -2.57467832 -2.35563408]
===============================================Measurements Collected
[array([-0.85731539, -1.15793805, -1.47062238, -1.77496243]), array([-1.47062238, -1.15793805, -0.85731539, -0.57287335]), array([-0.85731539, -1.15793805, -1.47062238, -1.77496243]), array([-2.06332538, -2.28201947, -2.39942104, -2.42722809]), array([-2.60679256, -2.6584211 , -2.57467832, -2.35563408])]
Base measurements collected
[array([-0.85731539, -1.15793805, -1.47062238, -1.77496243]), array([-1.47062238, -1.15793805, -0.85731539, -0.57287335]), array([-0.85731539, -1.15793805, -1.47062238, -1.77496243]), array([-2.06332538, -2.28201947, -2.39942104, -2.42722809]), array([-2.60679256, -2.6584211 , -2.57467832, -2.35563408])]
Total accumulated reward = -33.947945695
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-5.2608382425897577, -9.3195874020016767, -14.580425644591434, -23.752419632415851, -33.947945694962002]
Normalized Reward history [-3.2754832880623717, -5.3488774929469036, -8.6243607810092762, -15.810999814306308, -24.021170922325076]
