Physical State
[[ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.25  0.8 ]
 [ 1.3   0.8 ]
 [ 1.35  0.8 ]
 [ 1.4   0.8 ]
 [ 1.4   0.85]
 [ 1.4   0.9 ]
 [ 1.4   0.95]
 [ 1.4   1.  ]
 [ 1.45  1.  ]
 [ 1.5   1.  ]
 [ 1.55  1.  ]
 [ 1.6   1.  ]]
Measurements
[-0.57287335 -0.29449398 -0.01239151  0.27735296  0.57384111  0.37018718
  0.16797697 -0.00560217 -0.14502571  0.00240716  0.19445027  0.39634396
  0.61527654  0.90512644  1.16111411  1.34751026  1.43519994  1.48565321
  1.45097061  1.31392635  1.08596735]
===============================================Measurements Collected
[array([-0.29449398, -0.01239151,  0.27735296,  0.57384111]), array([ 0.37018718,  0.16797697, -0.00560217, -0.14502571]), array([ 0.00240716,  0.19445027,  0.39634396,  0.61527654]), array([ 0.90512644,  1.16111411,  1.34751026,  1.43519994]), array([ 1.48565321,  1.45097061,  1.31392635,  1.08596735])]
Base measurements collected
[array([-0.29449398, -0.01239151,  0.27735296,  0.57384111]), array([ 0.37018718,  0.16797697, -0.00560217, -0.14502571]), array([ 0.00240716,  0.19445027,  0.39634396,  0.61527654]), array([ 0.90512644,  1.16111411,  1.34751026,  1.43519994]), array([ 1.48565321,  1.45097061,  1.31392635,  1.08596735])]
Total accumulated reward = 12.3257910418
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.5443085846105078, 0.93184484660580702, 2.1403227838655945, 6.9892735291679671, 12.325791041799869]
Normalized Reward history [2.5296635391378937, 4.9025547556605789, 8.0963876474477523, 14.930693347277511, 22.252565814436799]
