Physical State
[[ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 0.95  1.2 ]
 [ 0.9   1.2 ]
 [ 0.85  1.2 ]
 [ 0.8   1.2 ]
 [ 0.8   1.25]
 [ 0.8   1.3 ]
 [ 0.8   1.35]
 [ 0.8   1.4 ]
 [ 0.85  1.4 ]
 [ 0.9   1.4 ]
 [ 0.95  1.4 ]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Measurements
[-0.45359257 -0.24476767 -0.09463167  0.06205431  0.26878746  0.28722092
  0.30936942  0.30294996  0.24286218  0.20485622  0.20632465  0.25066737
  0.33186644  0.59029205  0.84549532  1.08870205  1.31334053  1.4478163
  1.50638154  1.50063167  1.45665339]
===============================================Measurements Collected
[array([-0.24476767, -0.09463167,  0.06205431,  0.26878746]), array([ 0.28722092,  0.30936942,  0.30294996,  0.24286218]), array([ 0.20485622,  0.20632465,  0.25066737,  0.33186644]), array([ 0.59029205,  0.84549532,  1.08870205,  1.31334053]), array([ 1.4478163 ,  1.50638154,  1.50063167,  1.45665339])]
Base measurements collected
[array([-0.24476767, -0.09463167,  0.06205431,  0.26878746]), array([ 0.28722092,  0.30936942,  0.30294996,  0.24286218]), array([ 0.20485622,  0.20632465,  0.25066737,  0.33186644]), array([ 0.59029205,  0.84549532,  1.08870205,  1.31334053]), array([ 1.4478163 ,  1.50638154,  1.50063167,  1.45665339])]
Total accumulated reward = 11.8768724342
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-0.0085575752699748597, 1.1338448998148782, 2.127559576317652, 5.9653895271421344, 11.87687243424087]
Normalized Reward history [0.37203747052208636, 1.8950349913990008, 3.2693447136938358, 7.4877697103103795, 13.779847663201176]
