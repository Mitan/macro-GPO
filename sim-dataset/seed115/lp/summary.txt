Physical State
[[ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]]
Measurements
[-0.45359257 -0.74304312 -1.1329612  -1.59332432 -2.05109181 -2.20258121
 -2.36057096 -2.45695933 -2.46757816 -2.08752701 -1.68398081 -1.32443165
 -1.00999956 -0.90796206 -0.77012315 -0.60244204 -0.45359257 -0.60244204
 -0.77012315 -0.90796206 -1.00999956]
===============================================Measurements Collected
[array([-0.74304312, -1.1329612 , -1.59332432, -2.05109181]), array([-2.20258121, -2.36057096, -2.45695933, -2.46757816]), array([-2.08752701, -1.68398081, -1.32443165, -1.00999956]), array([-0.90796206, -0.77012315, -0.60244204, -0.45359257]), array([-0.60244204, -0.77012315, -0.90796206, -1.00999956])]
Base measurements collected
[array([-0.74304312, -1.1329612 , -1.59332432, -2.05109181]), array([-2.20258121, -2.36057096, -2.45695933, -2.46757816]), array([-2.08752701, -1.68398081, -1.32443165, -1.00999956]), array([-0.90796206, -0.77012315, -0.60244204, -0.45359257]), array([-0.60244204, -0.77012315, -0.90796206, -1.00999956])]
Total accumulated reward = -27.1386957565
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-5.5204204382837947, -15.008110100591358, -21.114049129508238, -23.848168949026018, -27.13869575651162]
Normalized Reward history [-5.1398253924917334, -14.246920009007235, -19.972263992132056, -22.325788765857776, -25.235720527551315]
