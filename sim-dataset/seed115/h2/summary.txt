Physical State
[[ 0.8   1.25]
 [ 0.8   1.3 ]
 [ 0.8   1.35]
 [ 0.8   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   0.95]
 [ 0.8   0.9 ]
 [ 0.8   0.85]
 [ 0.8   0.8 ]
 [ 0.8   0.85]
 [ 0.8   0.9 ]
 [ 0.8   0.95]
 [ 0.8   1.  ]
 [ 0.8   1.05]
 [ 0.8   1.1 ]
 [ 0.8   1.15]
 [ 0.8   1.2 ]
 [ 0.8   1.25]
 [ 0.8   1.3 ]
 [ 0.8   1.35]
 [ 0.8   1.4 ]]
Measurements
[-0.45359257 -0.29131239 -0.16667568 -0.08161326 -0.06931541 -0.40445289
 -0.82941172 -1.27188599 -1.66963729 -1.27188599 -0.82941172 -0.40445289
 -0.06931541  0.15504229  0.25389476  0.26467744  0.24286218  0.20485622
  0.20632465  0.25066737  0.33186644]
===============================================Measurements Collected
[array([-0.29131239, -0.16667568, -0.08161326, -0.06931541]), array([-0.40445289, -0.82941172, -1.27188599, -1.66963729]), array([-1.27188599, -0.82941172, -0.40445289, -0.06931541]), array([ 0.15504229,  0.25389476,  0.26467744,  0.24286218]), array([ 0.20485622,  0.20632465,  0.25066737,  0.33186644])]
Base measurements collected
[array([-0.29131239, -0.16667568, -0.08161326, -0.06931541]), array([-0.40445289, -0.82941172, -1.27188599, -1.66963729]), array([-1.27188599, -0.82941172, -0.40445289, -0.06931541]), array([ 0.15504229,  0.25389476,  0.26467744,  0.24286218]), array([ 0.20485622,  0.20632465,  0.25066737,  0.33186644])]
Total accumulated reward = -5.44917930538
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-0.60891674546431818, -4.7843046364706217, -7.3593706507222159, -6.4428939818828113, -5.4491793053800368]
Normalized Reward history [-0.22832169967225696, -4.0231145448864991, -6.217585513346032, -4.9205137987145662, -3.5462040764197309]
