Physical State
[[ 0.75  1.2 ]
 [ 0.7   1.2 ]
 [ 0.65  1.2 ]
 [ 0.6   1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.85]
 [ 0.8   0.9 ]
 [ 0.8   0.95]
 [ 0.8   1.  ]
 [ 0.8   1.05]
 [ 0.8   1.1 ]
 [ 0.8   1.15]
 [ 0.8   1.2 ]
 [ 0.75  1.2 ]
 [ 0.7   1.2 ]
 [ 0.65  1.2 ]
 [ 0.6   1.2 ]]
Measurements
[-0.45359257 -0.74304312 -1.1329612  -1.59332432 -2.05109181 -1.89794708
 -1.78984713 -1.71373101 -1.66963729 -1.27188599 -0.82941172 -0.40445289
 -0.06931541  0.15504229  0.25389476  0.26467744  0.24286218  0.09734304
 -0.11939043 -0.39297534 -0.66761521]
===============================================Measurements Collected
[array([-0.74304312, -1.1329612 , -1.59332432, -2.05109181]), array([-1.89794708, -1.78984713, -1.71373101, -1.66963729]), array([-1.27188599, -0.82941172, -0.40445289, -0.06931541]), array([ 0.15504229,  0.25389476,  0.26467744,  0.24286218]), array([ 0.09734304, -0.11939043, -0.39297534, -0.66761521])]
Base measurements collected
[array([-0.74304312, -1.1329612 , -1.59332432, -2.05109181]), array([-1.89794708, -1.78984713, -1.71373101, -1.66963729]), array([-1.27188599, -0.82941172, -0.40445289, -0.06931541]), array([ 0.15504229,  0.25389476,  0.26467744,  0.24286218]), array([ 0.09734304, -0.11939043, -0.39297534, -0.66761521])]
Total accumulated reward = -15.3328102347
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [-5.5204204382837947, -12.591582948343856, -15.166648962595449, -14.250172293756044, -15.332810234673168]
Normalized Reward history [-5.1398253924917334, -11.830392856759733, -14.024863825219267, -12.727792110587801, -13.429835005712864]
