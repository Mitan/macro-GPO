Physical State
[[ 1.2   1.25]
 [ 1.2   1.3 ]
 [ 1.2   1.35]
 [ 1.2   1.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.2   0.85]
 [ 1.2   0.9 ]
 [ 1.2   0.95]
 [ 1.2   1.  ]
 [ 1.2   1.05]
 [ 1.2   1.1 ]
 [ 1.2   1.15]
 [ 1.2   1.2 ]
 [ 1.2   1.25]
 [ 1.2   1.3 ]
 [ 1.2   1.35]
 [ 1.2   1.4 ]]
Measurements
[-0.45359257 -0.60244204 -0.77012315 -0.90796206 -1.00999956 -1.32443165
 -1.68398081 -2.08752701 -2.46757816 -2.08752701 -1.68398081 -1.32443165
 -1.00999956 -0.73002794 -0.42818338 -0.09622837  0.28415332  0.69520805
  1.06950807  1.36907506  1.54167834]
===============================================Measurements Collected
[array([-0.60244204, -0.77012315, -0.90796206, -1.00999956]), array([-1.32443165, -1.68398081, -2.08752701, -2.46757816]), array([-2.08752701, -1.68398081, -1.32443165, -1.00999956]), array([-0.73002794, -0.42818338, -0.09622837,  0.28415332]), array([ 0.69520805,  1.06950807,  1.36907506,  1.54167834])]
Base measurements collected
[array([-0.60244204, -0.77012315, -0.90796206, -1.00999956]), array([-1.32443165, -1.68398081, -2.08752701, -2.46757816]), array([-2.08752701, -1.68398081, -1.32443165, -1.00999956]), array([-0.73002794, -0.42818338, -0.09622837,  0.28415332]), array([ 0.69520805,  1.06950807,  1.36907506,  1.54167834])]
Total accumulated reward = -13.2548003061
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-3.2905268074856004, -10.854044442258783, -16.959983471175665, -17.930269832718235, -13.254800306143032]
Normalized Reward history [-2.9099317616935392, -10.09285435067466, -15.818198333799481, -16.40788964954999, -11.351825077182726]
