Physical State
[[ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.15  0.8 ]
 [ 1.1   0.8 ]
 [ 1.05  0.8 ]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]]
Measurements
[-0.45359257 -0.60244204 -0.77012315 -0.90796206 -1.00999956 -1.32443165
 -1.68398081 -2.08752701 -2.46757816 -2.45695933 -2.36057096 -2.20258121
 -2.05109181 -1.59332432 -1.1329612  -0.74304312 -0.45359257 -0.24476767
 -0.09463167  0.06205431  0.26878746]
===============================================Measurements Collected
[array([-0.60244204, -0.77012315, -0.90796206, -1.00999956]), array([-1.32443165, -1.68398081, -2.08752701, -2.46757816]), array([-2.45695933, -2.36057096, -2.20258121, -2.05109181]), array([-1.59332432, -1.1329612 , -0.74304312, -0.45359257]), array([-0.24476767, -0.09463167,  0.06205431,  0.26878746])]
Base measurements collected
[array([-0.60244204, -0.77012315, -0.90796206, -1.00999956]), array([-1.32443165, -1.68398081, -2.08752701, -2.46757816]), array([-2.45695933, -2.36057096, -2.20258121, -2.05109181]), array([-1.59332432, -1.1329612 , -0.74304312, -0.45359257]), array([-0.24476767, -0.09463167,  0.06205431,  0.26878746])]
Total accumulated reward = -23.8567265243
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-3.2905268074856004, -10.854044442258783, -19.925247748525166, -23.848168949026018, -23.856726524295993]
Normalized Reward history [-2.9099317616935392, -10.09285435067466, -18.783462611148984, -22.325788765857773, -21.953751295335685]
