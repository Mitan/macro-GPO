Physical State
[[ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]
 [ 1.    1.55]
 [ 1.    1.5 ]
 [ 1.    1.45]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Measurements
[-0.45359257 -0.24476767 -0.09463167  0.06205431  0.26878746  0.52048253
  0.81584904  1.08306574  1.31334053  1.4478163   1.50638154  1.50063167
  1.45665339  1.50063167  1.50638154  1.4478163   1.31334053  1.4478163
  1.50638154  1.50063167  1.45665339]
===============================================Measurements Collected
[array([-0.24476767, -0.09463167,  0.06205431,  0.26878746]), array([ 0.52048253,  0.81584904,  1.08306574,  1.31334053]), array([ 1.4478163 ,  1.50638154,  1.50063167,  1.45665339]), array([ 1.50063167,  1.50638154,  1.4478163 ,  1.31334053]), array([ 1.4478163 ,  1.50638154,  1.50063167,  1.45665339])]
Base measurements collected
[array([-0.24476767, -0.09463167,  0.06205431,  0.26878746]), array([ 0.52048253,  0.81584904,  1.08306574,  1.31334053]), array([ 1.4478163 ,  1.50638154,  1.50063167,  1.45665339]), array([ 1.50063167,  1.50638154,  1.4478163 ,  1.31334053]), array([ 1.4478163 ,  1.50638154,  1.50063167,  1.45665339])]
Total accumulated reward = 21.3153161234
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-0.0085575752699748597, 3.7241802625183609, 9.6356631696170965, 15.403833216294329, 21.315316123393064]
Normalized Reward history [0.37203747052208636, 4.4853703541024839, 10.77744830699328, 16.926213399462576, 23.218291352353372]
