Physical State
[[ 0.6   0.55]
 [ 0.6   0.5 ]
 [ 0.6   0.45]
 [ 0.6   0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]
 [ 0.75  0.6 ]
 [ 0.7   0.6 ]
 [ 0.65  0.6 ]
 [ 0.6   0.6 ]
 [ 0.6   0.55]
 [ 0.6   0.5 ]
 [ 0.6   0.45]
 [ 0.6   0.4 ]]
Measurements
[ 0.13392757  0.09314915  0.09312871  0.1408505   0.20596513  0.2831271
  0.36829105  0.43135899  0.47949149  0.71444126  1.01368338  1.31825807
  1.56483065  1.67608839  1.63073078  1.41133003  1.07696858  1.36894011
  1.613591    1.77804818  1.82983862]
===============================================Measurements Collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.2831271 ,  0.36829105,  0.43135899,  0.47949149]), array([ 0.71444126,  1.01368338,  1.31825807,  1.56483065]), array([ 1.67608839,  1.63073078,  1.41133003,  1.07696858]), array([ 1.36894011,  1.613591  ,  1.77804818,  1.82983862])]
Base measurements collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.2831271 ,  0.36829105,  0.43135899,  0.47949149]), array([ 0.71444126,  1.01368338,  1.31825807,  1.56483065]), array([ 1.67608839,  1.63073078,  1.41133003,  1.07696858]), array([ 1.36894011,  1.613591  ,  1.77804818,  1.82983862])]
Total accumulated reward = 19.0921111655
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [0.53309348805645884, 2.0953621188779423, 6.7065754770259591, 12.501693258577179, 19.092111165466711]
Normalized Reward history [-0.60539294963690371, -0.18161075650878278, 3.2911161639458717, 7.9477475078037285, 13.399678976999898]
