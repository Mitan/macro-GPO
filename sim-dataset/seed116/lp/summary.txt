Physical State
[[ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]]
Measurements
[ 0.13392757  0.09314915  0.09312871  0.1408505   0.20596513  0.1408505
  0.09312871  0.09314915  0.13392757  0.09314915  0.09312871  0.1408505
  0.20596513  0.1408505   0.09312871  0.09314915  0.13392757  0.09314915
  0.09312871  0.1408505   0.20596513]
===============================================Measurements Collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.1408505 ,  0.09312871,  0.09314915,  0.13392757]), array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.1408505 ,  0.09312871,  0.09314915,  0.13392757]), array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513])]
Base measurements collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.1408505 ,  0.09312871,  0.09314915,  0.13392757]), array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.1408505 ,  0.09312871,  0.09314915,  0.13392757]), array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513])]
Total accumulated reward = 2.52139233147
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.53309348805645884, 0.99414942170731391, 1.5272429097637727, 1.9882988434146278, 2.5213923314710867]
Normalized Reward history [-0.60539294963690371, -1.2828234536794112, -1.8882164033163149, -2.5656469073588224, -3.1710398569957263]
