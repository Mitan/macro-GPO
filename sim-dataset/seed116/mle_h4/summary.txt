Physical State
[[ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.85]
 [ 1.    0.9 ]
 [ 1.    0.95]
 [ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Measurements
[ 0.13392757  0.09314915  0.09312871  0.1408505   0.20596513  0.1408505
  0.09312871  0.09314915  0.13392757  0.23807734  0.40808157  0.65491022
  0.97370042  1.32522161  1.68200952  2.01582489  2.27018206  2.42582464
  2.48612988  2.43336989  2.30285844]
===============================================Measurements Collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.1408505 ,  0.09312871,  0.09314915,  0.13392757]), array([ 0.23807734,  0.40808157,  0.65491022,  0.97370042]), array([ 1.32522161,  1.68200952,  2.01582489,  2.27018206]), array([ 2.42582464,  2.48612988,  2.43336989,  2.30285844])]
Base measurements collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.1408505 ,  0.09312871,  0.09314915,  0.13392757]), array([ 0.23807734,  0.40808157,  0.65491022,  0.97370042]), array([ 1.32522161,  1.68200952,  2.01582489,  2.27018206]), array([ 2.42582464,  2.48612988,  2.43336989,  2.30285844])]
Total accumulated reward = 20.2103399158
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.53309348805645884, 0.99414942170731391, 3.268918968368018, 10.562157061922928, 20.210339915813329]
Normalized Reward history [-0.60539294963690371, -1.2828234536794112, -0.14654034471206945, 6.0082113111494779, 14.517907727346515]
