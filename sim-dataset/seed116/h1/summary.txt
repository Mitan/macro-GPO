Physical State
[[ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]
 [ 1.    1.55]
 [ 1.    1.5 ]
 [ 1.    1.45]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Measurements
[ 0.13392757  0.23807734  0.40808157  0.65491022  0.97370042  1.32522161
  1.68200952  2.01582489  2.27018206  2.42582464  2.48612988  2.43336989
  2.30285844  2.43336989  2.48612988  2.42582464  2.27018206  2.42582464
  2.48612988  2.43336989  2.30285844]
===============================================Measurements Collected
[array([ 0.23807734,  0.40808157,  0.65491022,  0.97370042]), array([ 1.32522161,  1.68200952,  2.01582489,  2.27018206]), array([ 2.42582464,  2.48612988,  2.43336989,  2.30285844]), array([ 2.43336989,  2.48612988,  2.42582464,  2.27018206]), array([ 2.42582464,  2.48612988,  2.43336989,  2.30285844])]
Base measurements collected
[array([ 0.23807734,  0.40808157,  0.65491022,  0.97370042]), array([ 1.32522161,  1.68200952,  2.01582489,  2.27018206]), array([ 2.42582464,  2.48612988,  2.43336989,  2.30285844]), array([ 2.43336989,  2.48612988,  2.42582464,  2.27018206]), array([ 2.42582464,  2.48612988,  2.43336989,  2.30285844])]
Total accumulated reward = 38.4798798206
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [2.2747695466607043, 9.5680076402156153, 19.216190494106016, 28.831696966741344, 38.479879820631744]
Normalized Reward history [1.1362831089673417, 7.2910347648288889, 15.800731181025927, 24.277751215967893, 32.787447632164927]
