Physical State
[[ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.05  0.8 ]
 [ 1.1   0.8 ]
 [ 1.15  0.8 ]
 [ 1.2   0.8 ]
 [ 1.2   0.75]
 [ 1.2   0.7 ]
 [ 1.2   0.65]
 [ 1.2   0.6 ]
 [ 1.15  0.6 ]
 [ 1.1   0.6 ]
 [ 1.05  0.6 ]
 [ 1.    0.6 ]
 [ 0.95  0.6 ]
 [ 0.9   0.6 ]
 [ 0.85  0.6 ]
 [ 0.8   0.6 ]]
Measurements
[ 0.13392757  0.09314915  0.09312871  0.1408505   0.20596513  0.27888784
  0.36498486  0.42443206  0.41648721  0.41295776  0.3621361   0.27516295
  0.16514132  0.20572793  0.24404256  0.32411396  0.47949149  0.71444126
  1.01368338  1.31825807  1.56483065]
===============================================Measurements Collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.27888784,  0.36498486,  0.42443206,  0.41648721]), array([ 0.41295776,  0.3621361 ,  0.27516295,  0.16514132]), array([ 0.20572793,  0.24404256,  0.32411396,  0.47949149]), array([ 0.71444126,  1.01368338,  1.31825807,  1.56483065])]
Base measurements collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.27888784,  0.36498486,  0.42443206,  0.41648721]), array([ 0.41295776,  0.3621361 ,  0.27516295,  0.16514132]), array([ 0.20572793,  0.24404256,  0.32411396,  0.47949149]), array([ 0.71444126,  1.01368338,  1.31825807,  1.56483065])]
Total accumulated reward = 9.09787287467
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.53309348805645884, 2.0178854638900479, 3.2332835811994594, 4.4866595165171894, 9.0978728746652067]
Normalized Reward history [-0.60539294963690371, -0.25908741149667724, -0.18217573188062808, -0.067286234256260125, 3.4054406861983946]
