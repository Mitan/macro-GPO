Physical State
[[ 0.6   0.55]
 [ 0.6   0.5 ]
 [ 0.6   0.45]
 [ 0.6   0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]
 [ 0.75  0.6 ]
 [ 0.7   0.6 ]
 [ 0.65  0.6 ]
 [ 0.6   0.6 ]
 [ 0.6   0.55]
 [ 0.6   0.5 ]
 [ 0.6   0.45]
 [ 0.6   0.4 ]]
Measurements
[ 0.13392757  0.09314915  0.09312871  0.1408505   0.20596513  0.18985637
  0.23683227  0.33437074  0.43028216  0.68165024  0.98365176  1.28877179
  1.56483065  1.67608839  1.63073078  1.41133003  1.07696858  1.36894011
  1.613591    1.77804818  1.82983862]
===============================================Measurements Collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.18985637,  0.23683227,  0.33437074,  0.43028216]), array([ 0.68165024,  0.98365176,  1.28877179,  1.56483065]), array([ 1.67608839,  1.63073078,  1.41133003,  1.07696858]), array([ 1.36894011,  1.613591  ,  1.77804818,  1.82983862])]
Base measurements collected
[array([ 0.09314915,  0.09312871,  0.1408505 ,  0.20596513]), array([ 0.18985637,  0.23683227,  0.33437074,  0.43028216]), array([ 0.68165024,  0.98365176,  1.28877179,  1.56483065]), array([ 1.67608839,  1.63073078,  1.41133003,  1.07696858]), array([ 1.36894011,  1.613591  ,  1.77804818,  1.82983862])]
Total accumulated reward = 18.628875172
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [0.53309348805645884, 1.7244350372304083, 6.243339483601142, 12.038457265152362, 18.628875172041894]
Normalized Reward history [-0.60539294963690371, -0.55253783815631685, 2.8278801705210537, 7.4845115143789105, 12.936442983575079]
