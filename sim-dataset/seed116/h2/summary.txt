Physical State
[[ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   0.95]
 [ 0.8   0.9 ]
 [ 0.8   0.85]
 [ 0.8   0.8 ]
 [ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]]
Measurements
[ 0.13392757  0.14110773  0.14119077  0.09093209 -0.00436149  0.09093209
  0.14119077  0.14110773  0.13392757  0.13256205  0.14403215  0.1731019
  0.19741744  0.13891643  0.15554768  0.24865882  0.43028216  0.68165024
  0.98365176  1.28877179  1.56483065]
===============================================Measurements Collected
[array([ 0.14110773,  0.14119077,  0.09093209, -0.00436149]), array([ 0.09093209,  0.14119077,  0.14110773,  0.13392757]), array([ 0.13256205,  0.14403215,  0.1731019 ,  0.19741744]), array([ 0.13891643,  0.15554768,  0.24865882,  0.43028216]), array([ 0.68165024,  0.98365176,  1.28877179,  1.56483065])]
Base measurements collected
[array([ 0.14110773,  0.14119077,  0.09093209, -0.00436149]), array([ 0.09093209,  0.14119077,  0.14110773,  0.13392757]), array([ 0.13256205,  0.14403215,  0.1731019 ,  0.19741744]), array([ 0.13891643,  0.15554768,  0.24865882,  0.43028216]), array([ 0.68165024,  0.98365176,  1.28877179,  1.56483065])]
Total accumulated reward = 7.0154503363
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.36886910373235521, 0.87602726852114632, 1.5231408036398659, 2.4965458899326407, 7.015450336303374]
Normalized Reward history [-0.76961733396100729, -1.4009456068655788, -1.8923185094402215, -2.0573998608408095, 1.323018147836561]
