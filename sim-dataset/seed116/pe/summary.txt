Physical State
[[ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.05  1.2 ]
 [ 1.1   1.2 ]
 [ 1.15  1.2 ]
 [ 1.2   1.2 ]
 [ 1.2   1.25]
 [ 1.2   1.3 ]
 [ 1.2   1.35]
 [ 1.2   1.4 ]
 [ 1.15  1.4 ]
 [ 1.1   1.4 ]
 [ 1.05  1.4 ]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]]
Measurements
[ 0.13392757  0.23807734  0.40808157  0.65491022  0.97370042  0.92555066
  0.84645757  0.72174976  0.58538608  0.94342231  1.27732259  1.53420593
  1.68752736  1.92661857  2.12693071  2.25595906  2.27018206  2.42582464
  2.48612988  2.43336989  2.30285844]
===============================================Measurements Collected
[array([ 0.23807734,  0.40808157,  0.65491022,  0.97370042]), array([ 0.92555066,  0.84645757,  0.72174976,  0.58538608]), array([ 0.94342231,  1.27732259,  1.53420593,  1.68752736]), array([ 1.92661857,  2.12693071,  2.25595906,  2.27018206]), array([ 2.42582464,  2.48612988,  2.43336989,  2.30285844])]
Base measurements collected
[array([ 0.23807734,  0.40808157,  0.65491022,  0.97370042]), array([ 0.92555066,  0.84645757,  0.72174976,  0.58538608]), array([ 0.94342231,  1.27732259,  1.53420593,  1.68752736]), array([ 1.92661857,  2.12693071,  2.25595906,  2.27018206]), array([ 2.42582464,  2.48612988,  2.43336989,  2.30285844])]
Total accumulated reward = 29.0242650522
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [2.2747695466607043, 5.3539136123443498, 10.796391798129804, 19.376082198268151, 29.024265052158551]
Normalized Reward history [1.1362831089673417, 3.0769407369576247, 7.3809324850497156, 14.822136447494698, 23.331832863691737]
