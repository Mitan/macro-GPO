Physical State
[[ 1.    0.55]
 [ 1.    0.5 ]
 [ 1.    0.45]
 [ 1.    0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.15]
 [ 1.    1.1 ]
 [ 1.    1.05]
 [ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.55]
 [ 1.    0.5 ]
 [ 1.    0.45]
 [ 1.    0.4 ]]
Measurements
[-0.13806479 -0.30261504 -0.46364023 -0.58459824 -0.66194241 -0.58459824
 -0.46364023 -0.30261504 -0.13806479  0.01763272  0.1384397   0.21915198
  0.29321246  0.38654575  0.53150743  0.7346214   0.96995698  1.19567735
  1.36498518  1.43784653  1.3995884 ]
===============================================Measurements Collected
[array([-0.30261504, -0.46364023, -0.58459824, -0.66194241]), array([-0.58459824, -0.46364023, -0.30261504, -0.13806479]), array([ 0.01763272,  0.1384397 ,  0.21915198,  0.29321246]), array([ 0.38654575,  0.53150743,  0.7346214 ,  0.96995698]), array([ 1.19567735,  1.36498518,  1.43784653,  1.3995884 ])]
Base measurements collected
[array([-0.30261504, -0.46364023, -0.58459824, -0.66194241]), array([-0.58459824, -0.46364023, -0.30261504, -0.13806479]), array([ 0.01763272,  0.1384397 ,  0.21915198,  0.29321246]), array([ 0.38654575,  0.53150743,  0.7346214 ,  0.96995698]), array([ 1.19567735,  1.36498518,  1.43784653,  1.3995884 ])]
Total accumulated reward = 5.18745167255
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-2.0127959119828667, -3.5017142054656394, -2.833277346420993, -0.21064578381982679, 5.1874516725477697]
Normalized Reward history [-2.0843111795259954, -3.6447447405518969, -3.0478231490503798, -0.49670685399234227, 4.8298753348321259]
