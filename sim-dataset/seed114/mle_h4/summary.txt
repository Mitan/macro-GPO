Physical State
[[ 1.    0.55]
 [ 1.    0.5 ]
 [ 1.    0.45]
 [ 1.    0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 1.    0.75]
 [ 1.    0.7 ]
 [ 1.    0.65]
 [ 1.    0.6 ]
 [ 1.    0.55]
 [ 1.    0.5 ]
 [ 1.    0.45]
 [ 1.    0.4 ]
 [ 1.    0.45]
 [ 1.    0.5 ]
 [ 1.    0.55]
 [ 1.    0.6 ]
 [ 1.    0.55]
 [ 1.    0.5 ]
 [ 1.    0.45]
 [ 1.    0.4 ]]
Measurements
[-0.13806479  0.01763272  0.1384397   0.21915198  0.29321246  0.38654575
  0.53150743  0.7346214   0.96995698  1.19567735  1.36498518  1.43784653
  1.3995884   1.43784653  1.36498518  1.19567735  0.96995698  1.19567735
  1.36498518  1.43784653  1.3995884 ]
===============================================Measurements Collected
[array([ 0.01763272,  0.1384397 ,  0.21915198,  0.29321246]), array([ 0.38654575,  0.53150743,  0.7346214 ,  0.96995698]), array([ 1.19567735,  1.36498518,  1.43784653,  1.3995884 ]), array([ 1.43784653,  1.36498518,  1.19567735,  0.96995698]), array([ 1.19567735,  1.36498518,  1.43784653,  1.3995884 ])]
Base measurements collected
[array([ 0.01763272,  0.1384397 ,  0.21915198,  0.29321246]), array([ 0.38654575,  0.53150743,  0.7346214 ,  0.96995698]), array([ 1.19567735,  1.36498518,  1.43784653,  1.3995884 ]), array([ 1.43784653,  1.36498518,  1.19567735,  0.96995698]), array([ 1.19567735,  1.36498518,  1.43784653,  1.3995884 ])]
Total accumulated reward = 19.055729369
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [0.66843685904464611, 3.2910684216458126, 8.6891658780134087, 13.657631912658148, 19.055729369025745]
Normalized Reward history [0.59692159150151725, 3.1480378865595546, 8.4746200753840224, 13.371570842485633, 18.698153031310103]
