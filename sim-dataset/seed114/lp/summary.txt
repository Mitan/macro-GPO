Physical State
[[ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.15  1.  ]
 [ 1.1   1.  ]
 [ 1.05  1.  ]
 [ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]]
Measurements
[-0.13806479  0.13538869  0.36340943  0.50793252  0.53657345  0.50793252
  0.36340943  0.13538869 -0.13806479  0.13538869  0.36340943  0.50793252
  0.53657345  0.50793252  0.36340943  0.13538869 -0.13806479  0.13538869
  0.36340943  0.50793252  0.53657345]
===============================================Measurements Collected
[array([ 0.13538869,  0.36340943,  0.50793252,  0.53657345]), array([ 0.50793252,  0.36340943,  0.13538869, -0.13806479]), array([ 0.13538869,  0.36340943,  0.50793252,  0.53657345]), array([ 0.50793252,  0.36340943,  0.13538869, -0.13806479]), array([ 0.13538869,  0.36340943,  0.50793252,  0.53657345])]
Base measurements collected
[array([ 0.13538869,  0.36340943,  0.50793252,  0.53657345]), array([ 0.50793252,  0.36340943,  0.13538869, -0.13806479]), array([ 0.13538869,  0.36340943,  0.50793252,  0.53657345]), array([ 0.50793252,  0.36340943,  0.13538869, -0.13806479]), array([ 0.13538869,  0.36340943,  0.50793252,  0.53657345])]
Total accumulated reward = 6.36724395555
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [1.5433040897372508, 2.4119699329077915, 3.9552740226450425, 4.823939865815583, 6.3672439555528335]
Normalized Reward history [1.471788822194122, 2.268939397821534, 3.7407282200156562, 4.537878795643068, 6.0096676178371897]
