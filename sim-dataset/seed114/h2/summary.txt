Physical State
[[ 1.2   0.75]
 [ 1.2   0.7 ]
 [ 1.2   0.65]
 [ 1.2   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.05  1.  ]
 [ 1.1   1.  ]
 [ 1.15  1.  ]
 [ 1.2   1.  ]
 [ 1.2   1.05]
 [ 1.2   1.1 ]
 [ 1.2   1.15]
 [ 1.2   1.2 ]
 [ 1.2   1.15]
 [ 1.2   1.1 ]
 [ 1.2   1.05]
 [ 1.2   1.  ]
 [ 1.2   0.95]
 [ 1.2   0.9 ]
 [ 1.2   0.85]
 [ 1.2   0.8 ]
 [ 1.2   0.75]
 [ 1.2   0.7 ]
 [ 1.2   0.65]
 [ 1.2   0.6 ]]
Measurements
[-0.13806479  0.13538869  0.36340943  0.50793252  0.53657345  0.37552138
  0.14885168 -0.08920664 -0.32190834 -0.08920664  0.14885168  0.37552138
  0.53657345  0.61830847  0.62069169  0.56556129  0.51661816  0.50851017
  0.59517983  0.76838952  0.98009004]
===============================================Measurements Collected
[array([ 0.13538869,  0.36340943,  0.50793252,  0.53657345]), array([ 0.37552138,  0.14885168, -0.08920664, -0.32190834]), array([-0.08920664,  0.14885168,  0.37552138,  0.53657345]), array([ 0.61830847,  0.62069169,  0.56556129,  0.51661816]), array([ 0.50851017,  0.59517983,  0.76838952,  0.98009004])]
Base measurements collected
[array([ 0.13538869,  0.36340943,  0.50793252,  0.53657345]), array([ 0.37552138,  0.14885168, -0.08920664, -0.32190834]), array([-0.08920664,  0.14885168,  0.37552138,  0.53657345]), array([ 0.61830847,  0.62069169,  0.56556129,  0.51661816]), array([ 0.50851017,  0.59517983,  0.76838952,  0.98009004])]
Total accumulated reward = 7.80165122049
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [1.5433040897372508, 1.6565621745799035, 2.6283020507697574, 4.9494816642755097, 7.8016512204856312]
Normalized Reward history [1.471788822194122, 1.513531639493646, 2.4137562481403712, 4.6634205941029947, 7.4440748827699874]
