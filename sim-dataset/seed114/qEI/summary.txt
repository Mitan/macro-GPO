Physical State
[[ 0.75  0.4 ]
 [ 0.7   0.4 ]
 [ 0.65  0.4 ]
 [ 0.6   0.4 ]]
Locations
[[ 1.    1.  ]
 [ 1.    0.95]
 [ 1.    0.9 ]
 [ 1.    0.85]
 [ 1.    0.8 ]
 [ 0.95  0.8 ]
 [ 0.9   0.8 ]
 [ 0.85  0.8 ]
 [ 0.8   0.8 ]
 [ 0.8   0.75]
 [ 0.8   0.7 ]
 [ 0.8   0.65]
 [ 0.8   0.6 ]
 [ 0.8   0.55]
 [ 0.8   0.5 ]
 [ 0.8   0.45]
 [ 0.8   0.4 ]
 [ 0.75  0.4 ]
 [ 0.7   0.4 ]
 [ 0.65  0.4 ]
 [ 0.6   0.4 ]]
Measurements
[-0.13806479  0.01763272  0.1384397   0.21915198  0.29321246  0.03418128
 -0.21270456 -0.40862603 -0.5442768  -0.38671623 -0.15593439  0.12888524
  0.47081917  0.84179756  1.19157179  1.50888199  1.7587514   1.50088352
  1.10556288  0.63759838  0.20572167]
===============================================Measurements Collected
[array([ 0.01763272,  0.1384397 ,  0.21915198,  0.29321246]), array([ 0.03418128, -0.21270456, -0.40862603, -0.5442768 ]), array([-0.38671623, -0.15593439,  0.12888524,  0.47081917]), array([ 0.84179756,  1.19157179,  1.50888199,  1.7587514 ]), array([ 1.50088352,  1.10556288,  0.63759838,  0.20572167])]
Base measurements collected
[array([ 0.01763272,  0.1384397 ,  0.21915198,  0.29321246]), array([ 0.03418128, -0.21270456, -0.40862603, -0.5442768 ]), array([-0.38671623, -0.15593439,  0.12888524,  0.47081917]), array([ 0.84179756,  1.19157179,  1.50888199,  1.7587514 ]), array([ 1.50088352,  1.10556288,  0.63759838,  0.20572167])]
Total accumulated reward = 8.34483370769
Nodes Expanded per stage
[4, 4, 4, 4, 4]
Total nodes expanded = 20
Reward history [0.66843685904464611, -0.46298925605172814, -0.40593546721321644, 4.8950672625073706, 8.3448337076901478]
Normalized Reward history [0.59692159150151725, -0.60601979113798576, -0.62048126984260288, 4.6090061923348546, 7.987257369974504]
