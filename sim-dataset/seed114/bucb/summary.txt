Physical State
[[ 0.65  0.6 ]
 [ 0.7   0.6 ]
 [ 0.75  0.6 ]
 [ 0.8   0.6 ]]
Locations
[[ 1.    1.  ]
 [ 0.95  1.  ]
 [ 0.9   1.  ]
 [ 0.85  1.  ]
 [ 0.8   1.  ]
 [ 0.8   0.95]
 [ 0.8   0.9 ]
 [ 0.8   0.85]
 [ 0.8   0.8 ]
 [ 0.75  0.8 ]
 [ 0.7   0.8 ]
 [ 0.65  0.8 ]
 [ 0.6   0.8 ]
 [ 0.6   0.75]
 [ 0.6   0.7 ]
 [ 0.6   0.65]
 [ 0.6   0.6 ]
 [ 0.65  0.6 ]
 [ 0.7   0.6 ]
 [ 0.75  0.6 ]
 [ 0.8   0.6 ]]
Measurements
[-0.13806479 -0.4135574  -0.64415536 -0.78676608 -0.83199364 -0.77169587
 -0.71751275 -0.64270642 -0.5442768  -0.60883737 -0.6065552  -0.54830862
 -0.43587692 -0.56666803 -0.65975572 -0.69354001 -0.62268338 -0.40405003
 -0.10439145  0.20296041  0.47081917]
===============================================Measurements Collected
[array([-0.4135574 , -0.64415536, -0.78676608, -0.83199364]), array([-0.77169587, -0.71751275, -0.64270642, -0.5442768 ]), array([-0.60883737, -0.6065552 , -0.54830862, -0.43587692]), array([-0.56666803, -0.65975572, -0.69354001, -0.62268338]), array([-0.40405003, -0.10439145,  0.20296041,  0.47081917])]
Base measurements collected
[array([-0.4135574 , -0.64415536, -0.78676608, -0.83199364]), array([-0.77169587, -0.71751275, -0.64270642, -0.5442768 ]), array([-0.60883737, -0.6065552 , -0.54830862, -0.43587692]), array([-0.56666803, -0.65975572, -0.69354001, -0.62268338]), array([-0.40405003, -0.10439145,  0.20296041,  0.47081917])]
Total accumulated reward = -9.92955147896
Nodes Expanded per stage
[-1.0, -1.0, -1.0, -1.0, -1.0]
Total nodes expanded = -5.0
Reward history [-2.6764724804383468, -5.3526643214830472, -7.5522424346002293, -10.094889573545405, -9.9295514789592065]
Normalized Reward history [-2.7479877479814756, -5.4956948565693047, -7.7667882372296155, -10.38095064371792, -10.28712781667485]
