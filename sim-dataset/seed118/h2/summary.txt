Physical State
[[ 0.75  1.6 ]
 [ 0.7   1.6 ]
 [ 0.65  1.6 ]
 [ 0.6   1.6 ]]
Locations
[[ 1.    1.  ]
 [ 1.    1.05]
 [ 1.    1.1 ]
 [ 1.    1.15]
 [ 1.    1.2 ]
 [ 1.    1.25]
 [ 1.    1.3 ]
 [ 1.    1.35]
 [ 1.    1.4 ]
 [ 1.    1.45]
 [ 1.    1.5 ]
 [ 1.    1.55]
 [ 1.    1.6 ]
 [ 0.95  1.6 ]
 [ 0.9   1.6 ]
 [ 0.85  1.6 ]
 [ 0.8   1.6 ]
 [ 0.75  1.6 ]
 [ 0.7   1.6 ]
 [ 0.65  1.6 ]
 [ 0.6   1.6 ]]
Measurements
[-0.39498829 -0.4031062  -0.39538731 -0.36086325 -0.30117581 -0.19470092
 -0.04067966  0.16171762  0.38899331  0.61028119  0.790125    0.90063203
  0.9092285   1.01470895  1.11753153  1.2186365   1.31224012  1.4000992
  1.44680851  1.42276495  1.319051  ]
===============================================Measurements Collected
[array([-0.4031062 , -0.39538731, -0.36086325, -0.30117581]), array([-0.19470092, -0.04067966,  0.16171762,  0.38899331]), array([ 0.61028119,  0.790125  ,  0.90063203,  0.9092285 ]), array([ 1.01470895,  1.11753153,  1.2186365 ,  1.31224012]), array([ 1.4000992 ,  1.44680851,  1.42276495,  1.319051  ])]
Base measurements collected
[array([-0.4031062 , -0.39538731, -0.36086325, -0.30117581]), array([-0.19470092, -0.04067966,  0.16171762,  0.38899331]), array([ 0.61028119,  0.790125  ,  0.90063203,  0.9092285 ]), array([ 1.01470895,  1.11753153,  1.2186365 ,  1.31224012]), array([ 1.4000992 ,  1.44680851,  1.42276495,  1.319051  ])]
Total accumulated reward = 12.3169052609
Nodes Expanded per stage
[-1, -1, -1, -1, -1]
Total nodes expanded = -5
Reward history [-1.4605325672952953, -1.1452022109238755, 2.0650645092564752, 6.7281816077337115, 12.316905260940537]
Normalized Reward history [-1.108184637625691, -0.44050635158466678, 3.1221082982652884, 8.137573326412129, 14.078644909288558]
